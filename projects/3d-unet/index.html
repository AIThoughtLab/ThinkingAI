<!doctype html><html lang=en><head><title>3D UNet for Brain Tumor Segmentation Â· AI for HUMANITY</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Mohamed ABDUL GAFOOR"><meta name=description content="In this post we will discuss how to use a 3D UNet to train a deep learning model. 3D UNet is a deep learning technique used for volumetric image segmentation, which is the process of dividing a 3D image into multiple regions or segments based on their characteristics. The architecture of 3D UNet is based on the popular 2D UNet, which has been widely used in biomedical image segmentation. If you want to read the paper of 2D UNet, visit here"><meta name=keywords content="blog,developer,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="3D UNet for Brain Tumor Segmentation"><meta name=twitter:description content="In this post we will discuss how to use a 3D UNet to train a deep learning model. 3D UNet is a deep learning technique used for volumetric image segmentation, which is the process of dividing a 3D image into multiple regions or segments based on their characteristics. The architecture of 3D UNet is based on the popular 2D UNet, which has been widely used in biomedical image segmentation. If you want to read the paper of 2D UNet, visit here"><meta property="og:title" content="3D UNet for Brain Tumor Segmentation"><meta property="og:description" content="In this post we will discuss how to use a 3D UNet to train a deep learning model. 3D UNet is a deep learning technique used for volumetric image segmentation, which is the process of dividing a 3D image into multiple regions or segments based on their characteristics. The architecture of 3D UNet is based on the popular 2D UNet, which has been widely used in biomedical image segmentation. If you want to read the paper of 2D UNet, visit here"><meta property="og:type" content="article"><meta property="og:url" content="https://AIThoughtLab.github.io/ThinkingAI/projects/3d-unet/"><meta property="article:section" content="projects"><meta property="article:published_time" content="2022-05-05T00:00:00+00:00"><meta property="article:modified_time" content="2022-05-05T00:00:00+00:00"><link rel=canonical href=https://AIThoughtLab.github.io/ThinkingAI/projects/3d-unet/><link rel=preload href="https://AIThoughtLab.github.io/ThinkingAI/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=https://AIThoughtLab.github.io/ThinkingAI/css/coder.min.36f76aaf39a14ecf5c3a3c6250dcaf06c238b3d8365d17d646f95cb1874e852b.css integrity="sha256-NvdqrzmhTs9cOjxiUNyvBsI4s9g2XRfWRvlcsYdOhSs=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://AIThoughtLab.github.io/ThinkingAI/css/coder-dark.min.593028e7f7ac55c003b79c230d1cd411bb4ca53b31556c3abb7f027170e646e9.css integrity="sha256-WTAo5/esVcADt5wjDRzUEbtMpTsxVWw6u38CcXDmRuk=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=https://AIThoughtLab.github.io/ThinkingAI/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://AIThoughtLab.github.io/ThinkingAI/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=https://AIThoughtLab.github.io/ThinkingAI/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=https://AIThoughtLab.github.io/ThinkingAI/images/apple-touch-icon.png><link rel=manifest href=https://AIThoughtLab.github.io/ThinkingAI/site.webmanifest><link rel=mask-icon href=https://AIThoughtLab.github.io/ThinkingAI/images/safari-pinned-tab.svg color=#5bbad5><meta name=generator content="Hugo 0.111.2"></head><body class="preload-transitions colorscheme-dark"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://AIThoughtLab.github.io/ThinkingAI/>AI for HUMANITY</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/about/>About</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/innovations/>Innovations</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/personal/>Personal</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container page"><article><header><h1 class=title><a class=title-link href=https://AIThoughtLab.github.io/ThinkingAI/projects/3d-unet/>3D UNet for Brain Tumor Segmentation</a></h1></header><p>In this post we will discuss how to use a 3D UNet to train a deep learning model. 3D UNet is a deep learning technique used for volumetric image segmentation, which is the process of dividing a 3D image into multiple regions or segments based on their characteristics. The architecture of 3D UNet is based on the popular 2D UNet, which has been widely used in biomedical image segmentation. If you want to read the paper of 2D UNet, visit <a href=https://arxiv.org/abs/1505.04597>here</a></p><p>Similar to the 2D UNet architecture, 3D consists of an encoder and a decoder network, connected by a bottleneck layer. The encoder network down-samples the input volume by applying convolutional and max pooling layers to extract features at multiple scales, while the decoder network up-samples the feature maps to generate a segmentation map. The bottleneck layer connects the encoder and decoder networks and preserves the spatial information of the input volume.</p><p>3D UNet has been shown to achieve state-of-the-art performance in many medical image segmentation tasks, such as brain tumor segmentation, and cardiac segmentation. In this post we are going to demonstrate that 3D UNet can achieve state-of-the-art performance in various medical image segmentation tasks, including but not limited to brain tumor segmentation. The 3D UNet&rsquo;s capability to process 3D data makes it particularly suitable for analyzing medical images that are typically volumetric in nature.</p><p><strong>Brain Tumor Segmentation Challenge 2020 - BraTS2020</strong></p><p>BraTS2020 dataset is a collection of MRI (Magnetic Resonance Imaging) scans of the brain that have been annotated to aid in the development and evaluation of algorithms for brain tumor segmentation. The dataset was created as part of a challenge hosted by the Medical Image Computing and Computer Assisted Intervention (MICCAI) Society and consists of high-grade glioma and low-grade glioma tumor types, as well as healthy brain tissue.</p><p>The dataset includes images from multiple modalities, including T1-weighted, T1-weighted with gadolinium contrast, T2-weighted, and FLAIR (Fluid-Attenuated Inversion Recovery) sequences, as well as segmentation masks that indicate the location and type of tumors in the brain. The dataset is intended for use in the development and evaluation of deep learning algorithms for automated brain tumor segmentation, which has the potential to improve the accuracy and speed of diagnosis and treatment planning for patients with brain tumors.</p><p>Following is a visualization of a brain along 3 different projections.</p><figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/copy.gif></figure><p>Let&rsquo;s take a moment to visualize the various MRI sequences, including T1, T1ce, T2,Flair and mask, that are commonly used in medical imaging to assess the brain&rsquo;s anatomy and pathology.<figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/t1t1ce.png></figure></p><p>The images and masks in this dataset have a shape of (155, 240, 240), where 155 is the depth and 240 represents the height and width. However, we have cropped the data to (144, 224, 224). We have opted to crop the data rather than resize it because resizing could alter the pixel values. Since we do not want any changes in the pixel values, cropping has been our preferred choice.</p><p>Following is the code to create a 3D UNet architecture;</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>keras.layers</span> <span style=font-weight:700>import</span> Input, Conv3D, MaxPooling3D, UpSampling3D, concatenate
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>keras.models</span> <span style=font-weight:700>import</span> Model
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>keras.layers</span> <span style=font-weight:700>import</span> Dropout
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>tensorflow.keras</span> <span style=font-weight:700>import</span> regularizers
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>def</span> create_unet(num_layers, num_neurons, filter_size, dropout_rate, input_shape=(144, 224, 224, 4)):
</span></span><span style=display:flex><span>    inputs = Input(shape=input_shape)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=font-style:italic># Encoder</span>
</span></span><span style=display:flex><span>    conv_layers = []
</span></span><span style=display:flex><span>    pool_layers = []
</span></span><span style=display:flex><span>    x = inputs
</span></span><span style=display:flex><span>    <span style=font-weight:700>for</span> i <span style=font-weight:700>in</span> range(num_layers):
</span></span><span style=display:flex><span>        x = Conv3D(num_neurons * 2 ** i, filter_size, activation=<span style=font-style:italic>&#39;relu&#39;</span>, padding=<span style=font-style:italic>&#39;same&#39;</span>)(x)
</span></span><span style=display:flex><span>        x = Dropout(rate=dropout_rate)(x) <span style=font-style:italic># Add dropout layer</span>
</span></span><span style=display:flex><span>        x = Conv3D(num_neurons * 2 ** i, filter_size, activation=<span style=font-style:italic>&#39;relu&#39;</span>, padding=<span style=font-style:italic>&#39;same&#39;</span>)(x)
</span></span><span style=display:flex><span>        x = Dropout(rate=dropout_rate)(x) <span style=font-style:italic># Add dropout layer</span>
</span></span><span style=display:flex><span>        conv_layers.append(x)
</span></span><span style=display:flex><span>        <span style=font-weight:700>if</span> i &lt; num_layers - 1:
</span></span><span style=display:flex><span>            x = MaxPooling3D((2, 2, 2))(x)
</span></span><span style=display:flex><span>            pool_layers.append(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=font-style:italic># Decoder</span>
</span></span><span style=display:flex><span>    <span style=font-weight:700>for</span> i <span style=font-weight:700>in</span> range(num_layers - 1, -1, -1):
</span></span><span style=display:flex><span>        <span style=font-weight:700>if</span> i &lt; num_layers - 1:
</span></span><span style=display:flex><span>            x = UpSampling3D((2, 2, 2))(x)
</span></span><span style=display:flex><span>            x = concatenate([x, conv_layers[i]], axis=-1)
</span></span><span style=display:flex><span>        x = Conv3D(num_neurons * 2 ** i, filter_size, activation=<span style=font-style:italic>&#39;relu&#39;</span>, padding=<span style=font-style:italic>&#39;same&#39;</span>)(x)
</span></span><span style=display:flex><span>        x = Dropout(rate=dropout_rate)(x) <span style=font-style:italic># Add dropout layer</span>
</span></span><span style=display:flex><span>        x = Conv3D(num_neurons * 2 ** i, filter_size, activation=<span style=font-style:italic>&#39;relu&#39;</span>, padding=<span style=font-style:italic>&#39;same&#39;</span>)(x)
</span></span><span style=display:flex><span>        x = Dropout(rate=dropout_rate)(x) <span style=font-style:italic># Add dropout layer</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=font-style:italic># Output</span>
</span></span><span style=display:flex><span>    outputs = Conv3D(4, 1, activation=<span style=font-style:italic>&#39;softmax&#39;</span>)(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    model = Model(inputs=inputs, outputs=outputs)
</span></span><span style=display:flex><span>    <span style=font-weight:700>return</span> model
</span></span></code></pre></div><p>In this architecture, the input has a shape of (144, 224, 224, 4). Here, 144 represents the depth of the image, while 224 indicates its height and width as previously mentioned. Additionally, 4 represents the number of channels, with T1, T1ce, T2, and Flair serving as the four channels. Therefore, we stack all the input images to form (144, 224, 224, 4).</p><p>The model is created by first defining the input shape and then constructing the encoder and decoder layers. The encoder layers consist of multiple Conv3D layers followed by dropout layers to help prevent overfitting. The max pooling layers are also used to reduce the spatial resolution of the feature maps. The outputs of the encoder layers are saved in the conv_layers and pool_layers lists.</p><p>The decoder layers use the saved conv_layers to create an upsampling path that mirrors the encoding path. This path consists of Conv3D layers and dropout layers. The Conv3D layers have the same number of neurons as the corresponding encoding layers. The upsampling is achieved by using UpSampling3D layers to double the size of the feature maps at each step, followed by concatenation of the resulting tensor with the corresponding tensor from the encoder layer.</p><p>Finally, the output layer is a Conv3D layer with softmax activation, which generates the predicted segmentation maps. The segmentation map has pixel values of 0, 1, 2 and 4. But we have changed 4 to 3, so the pixel values were 0, 1, 2 & 3. The model is then created using the functional API of Keras, which takes the input and output layers and produces a model object that can be trained and used for making predictions.</p><p>Let us set a 4 layer network with filter size of 3 and drop out value of 0.25. See the model <a href="https://drive.google.com/file/d/17408kkQlkScTD26Z2pqe-NiwSVlX0B_2/view?usp=share_link">here.</a></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>model = create_unet(num_layers=4, num_neurons=16, filter_size=(3, 3, 3),dropout_rate = 0.25,input_shape=(144, 224, 224, 4))
</span></span></code></pre></div><p>Choosing a correct Metrics like <strong>Dice coefficient</strong> are important in deep learning to measure the performance of the model. In the context of medical image segmentation, Dice coefficient is a commonly used metric for evaluating how well the model has segmented the object of interest from the background. The Dice coefficient, also known as the F1 score, calculates the overlap between the predicted and true segmentation masks, taking into account both false positives and false negatives. It ranges from 0 to 1, where 1 represents a perfect match between the predicted and true masks. By monitoring the Dice coefficient during model training, we can see how well the model is learning to segment the object of interest. We can use this metric to compare the performance of different models, to select the best model, and to optimize the hyperparameters of the model.</p><p>In general, metrics help us to quantify the performance of the model in a meaningful way, and guide us to make improvements to the model or the data used to train the model.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=font-weight:700>def</span> dice_coeff(y_true, y_pred, epsilon=1e-6):
</span></span><span style=display:flex><span>    <span style=font-style:italic># Flatten the tensors to 2D and calculate the intersection and union</span>
</span></span><span style=display:flex><span>    y_true = tf.reshape(y_true, [-1])
</span></span><span style=display:flex><span>    y_pred = tf.reshape(y_pred, [-1])
</span></span><span style=display:flex><span>    intersection = tf.reduce_sum(y_true * y_pred)
</span></span><span style=display:flex><span>    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=font-style:italic># Calculate the dice coefficient and return it</span>
</span></span><span style=display:flex><span>    dc = (2.0 * intersection + epsilon) / (union + epsilon)
</span></span><span style=display:flex><span>    <span style=font-weight:700>return</span> dc
</span></span></code></pre></div><p>We first flatten the tensors to 2D using the tf.reshape function. Then, it calculates the intersection and union of the flattened tensors using the tf.reduce_sum function. The intersection is the number of elements that are both in the ground truth and predicted masks, while the union is the number of elements that are in either the ground truth or predicted masks. Next, the function calculates the Dice coefficient using the formula: (2 * intersection + epsilon) / (union + epsilon), where epsilon is a small number (1e-6) added to the denominator to avoid division by zero.</p><p>Similarly, we will also use <strong>Dice Loss</strong>, <strong>Intersection Over Union</strong>, <strong>Mean Intersection Over Union</strong>. Set the learning rate to 0.001 in Adam optimizer and loss to &ldquo;categorical_crossentropy&rdquo;, which is used to measure the difference between the predicted probability distribution (output) and the true probability distribution (target) for a multi-class classification problem. Segmentation problems is similar to pixel-wise classification.</p><p><strong>DataGenerator</strong></p><p>When working with large amounts of data such as 3D images and masks, it is not feasible to load all the data into memory at once, so we need a way to load the data in batches. This is where a data generator comes in. A data generator is a Python generator that yields batches of data on-the-fly during training. It loads the data from disk or other data sources, performs data augmentation and preprocessing, and then passes the data to the model for training. This allows us to efficiently work with large datasets that cannot fit into memory.</p><p>In addition to providing a way to load large datasets, data generators are also useful for data augmentation. Data augmentation is a technique where we create new data from existing data by applying transformations such as rotation, flipping, and scaling. By generating new data from the existing data, we can increase the size of our dataset and improve the generalization of the model.</p><p>Following is the data generator class in our case;</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=font-weight:700>import</span> <span style=font-weight:700>numpy</span> <span style=font-weight:700>as</span> <span style=font-weight:700>np</span>
</span></span><span style=display:flex><span><span style=font-weight:700>import</span> <span style=font-weight:700>os</span>
</span></span><span style=display:flex><span><span style=font-weight:700>import</span> <span style=font-weight:700>nibabel</span> <span style=font-weight:700>as</span> <span style=font-weight:700>nib</span>
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>tensorflow.python.framework.ops</span> <span style=font-weight:700>import</span> Tensor
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>tensorflow.keras.preprocessing.image</span> <span style=font-weight:700>import</span> ImageDataGenerator
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>class</span> <span style=font-weight:700>DataGenerator</span>(tf.keras.utils.Sequence):
</span></span><span style=display:flex><span>    <span style=font-weight:700>def</span> __init__(self, data_dir, batch_size=1, dim=(144, 224, 224), n_channels=4, n_classes=4, shuffle=<span style=font-weight:700>True</span>, augment=<span style=font-weight:700>True</span>):
</span></span><span style=display:flex><span>        self.data_dir = data_dir
</span></span><span style=display:flex><span>        self.dim = dim
</span></span><span style=display:flex><span>        self.batch_size = batch_size
</span></span><span style=display:flex><span>        self.n_channels = n_channels
</span></span><span style=display:flex><span>        self.n_classes = n_classes
</span></span><span style=display:flex><span>        self.shuffle = shuffle
</span></span><span style=display:flex><span>        self.augment = augment
</span></span><span style=display:flex><span>        self.files = os.listdir(data_dir)
</span></span><span style=display:flex><span>        self.on_epoch_end()
</span></span><span style=display:flex><span>        self.image_datagen = ImageDataGenerator(rotation_range=20, horizontal_flip=<span style=font-weight:700>True</span>, vertical_flip=<span style=font-weight:700>True</span>)
</span></span><span style=display:flex><span>        <span style=font-style:italic>#self.is_training = is_training   </span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=font-weight:700>def</span> __len__(self):
</span></span><span style=display:flex><span>        <span style=font-weight:700>return</span> int(np.floor(len(self.files) / self.batch_size))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=font-weight:700>def</span> __getitem__(self, index):
</span></span><span style=display:flex><span>        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]
</span></span><span style=display:flex><span>        files = [self.files[i] <span style=font-weight:700>for</span> i <span style=font-weight:700>in</span> indexes]
</span></span><span style=display:flex><span>        <span style=font-weight:700>return</span> self.__data_generation(files)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=font-weight:700>def</span> on_epoch_end(self):
</span></span><span style=display:flex><span>        self.indexes = np.arange(len(self.files))
</span></span><span style=display:flex><span>        <span style=font-weight:700>if</span> self.shuffle:
</span></span><span style=display:flex><span>            np.random.shuffle(self.indexes)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=font-weight:700>def</span> crop(self, mri_data, data_mri = <span style=font-weight:700>True</span>):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=font-style:italic># Define the cropping parameters</span>
</span></span><span style=display:flex><span>      start = (6, 8, 8)
</span></span><span style=display:flex><span>      end = (-5, -8, -8)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=font-style:italic># Crop the MRI data using the defined parameters</span>
</span></span><span style=display:flex><span>      data_cropped = mri_data[start[0]:end[0], start[1]:end[1], start[2]:end[2]]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=font-style:italic># return data_cropped</span>
</span></span><span style=display:flex><span>      <span style=font-weight:700>if</span> data_mri:
</span></span><span style=display:flex><span>        min_ = np.min(data_cropped)
</span></span><span style=display:flex><span>        data_cropped = (data_cropped - min_) / (np.max(data_cropped) - min_)
</span></span><span style=display:flex><span>        data_cropped = np.round(data_cropped, 3)
</span></span><span style=display:flex><span>        <span style=font-style:italic>#print(data_cropped.shape)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        data_cropped = gaussian_filter(data_cropped, sigma=(1, 1, 1)) <span style=font-style:italic># Apply Gaussian smoothing to reduce noise</span>
</span></span><span style=display:flex><span>        data_cropped = data_cropped - 0.3 * laplace(data_cropped)     <span style=font-style:italic># Apply the Laplacian filter to sharpen the image</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=font-weight:700>return</span> data_cropped
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=font-weight:700>else</span>:
</span></span><span style=display:flex><span>        <span style=font-weight:700>return</span> data_cropped
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=font-weight:700>def</span> __data_generation(self, files):
</span></span><span style=display:flex><span>       
</span></span><span style=display:flex><span>        <span style=font-style:italic># Initialization</span>
</span></span><span style=display:flex><span>        X = np.empty((self.batch_size, *self.dim, self.n_channels), dtype=np.float32) <span style=font-style:italic># input</span>
</span></span><span style=display:flex><span>        y = np.empty((self.batch_size, *self.dim, self.n_classes), dtype=np.float32) <span style=font-style:italic># output/labels</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-weight:700>for</span> i, file <span style=font-weight:700>in</span> enumerate(files):
</span></span><span style=display:flex><span>          
</span></span><span style=display:flex><span>          <span style=font-weight:700>if</span> <span style=font-weight:700>not</span> file.endswith(<span style=font-style:italic>&#34;.csv&#34;</span>):
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            directory = os.path.join(self.data_dir, file)
</span></span><span style=display:flex><span>            flair = nib.load(os.path.join(directory, file + <span style=font-style:italic>&#34;_flair.nii&#34;</span>)).get_fdata()
</span></span><span style=display:flex><span>            flair = np.moveaxis(flair, [2, 0, 1], [0, 1, 2])
</span></span><span style=display:flex><span>            flair = self.crop(flair, data_mri = <span style=font-weight:700>True</span>)
</span></span><span style=display:flex><span>                
</span></span><span style=display:flex><span>            t1 = nib.load(os.path.join(directory, file + <span style=font-style:italic>&#34;_t1.nii&#34;</span>)).get_fdata()
</span></span><span style=display:flex><span>            t1 = np.moveaxis(t1, [2, 0, 1], [0, 1, 2])
</span></span><span style=display:flex><span>            t1 = self.crop(t1, data_mri = <span style=font-weight:700>True</span>)
</span></span><span style=display:flex><span>           
</span></span><span style=display:flex><span>            t1ce = nib.load(os.path.join(directory, file + <span style=font-style:italic>&#34;_t1ce.nii&#34;</span>)).get_fdata()
</span></span><span style=display:flex><span>            t1ce = np.moveaxis(t1ce, [2, 0, 1], [0, 1, 2])
</span></span><span style=display:flex><span>            t1ce = self.crop(t1ce, data_mri = <span style=font-weight:700>True</span>)
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            t2 = nib.load(os.path.join(directory, file + <span style=font-style:italic>&#34;_t2.nii&#34;</span>)).get_fdata()
</span></span><span style=display:flex><span>            t2 = np.moveaxis(t2, [2, 0, 1], [0, 1, 2])
</span></span><span style=display:flex><span>            t2 = self.crop(t2, data_mri = <span style=font-weight:700>True</span>)
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=font-style:italic># load the mask</span>
</span></span><span style=display:flex><span>            mask = nib.load(os.path.join(directory, file + <span style=font-style:italic>&#34;_seg.nii&#34;</span>)).get_fdata()
</span></span><span style=display:flex><span>            mask = np.moveaxis(mask, [2, 0, 1], [0, 1, 2])
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            mask[mask == 4] = 3 <span style=font-style:italic># pixel value 4 to 3</span>
</span></span><span style=display:flex><span>            mask = self.crop(mask, data_mri = <span style=font-weight:700>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            mask = mask.astype(<span style=font-style:italic>&#39;int&#39;</span>)  <span style=font-style:italic># convert mask to integer data type</span>
</span></span><span style=display:flex><span>            <span style=font-style:italic>#print(&#34;mask shape: &#34;, mask.shape)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            num_classes = 4
</span></span><span style=display:flex><span>            mask = np.expand_dims(mask, axis=-1)
</span></span><span style=display:flex><span>            masks = tf.keras.utils.to_categorical(mask, num_classes)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=font-style:italic># create stack for images and one-hot for masks..</span>
</span></span><span style=display:flex><span>            images = np.stack([flair, t1, t1ce, t2], axis=-1)
</span></span><span style=display:flex><span>      
</span></span><span style=display:flex><span>            <span style=font-weight:700>if</span> self.augment:
</span></span><span style=display:flex><span>              <span style=font-style:italic># apply random rotation and flip</span>
</span></span><span style=display:flex><span>              seed = np.random.randint(1, 100)
</span></span><span style=display:flex><span>              datagen = ImageDataGenerator(rotation_range=20, horizontal_flip=<span style=font-weight:700>True</span>, vertical_flip=<span style=font-weight:700>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>              <span style=font-weight:700>for</span> k <span style=font-weight:700>in</span> range(len(images)):
</span></span><span style=display:flex><span>                images[k] = datagen.random_transform(images[k], seed=seed)
</span></span><span style=display:flex><span>                masks[k] = datagen.random_transform(masks[k], seed=seed)
</span></span><span style=display:flex><span>                masks = masks.astype(int) <span style=font-style:italic># convert to int type so we will have 0 and 1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>              X[i,] = images
</span></span><span style=display:flex><span>              y[i,] = masks
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=font-weight:700>else</span>:
</span></span><span style=display:flex><span>              X[i,] = images
</span></span><span style=display:flex><span>              y[i,] = masks
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-weight:700>return</span> X, y
</span></span></code></pre></div><ul><li><p>The <strong>len</strong> method: return the length of the object, which is typically the number of batches in the dataset.</p></li><li><p>The <strong>getitem</strong> method: takes an index as input and returns the data associated with that index. The index corresponds to a specific batch of data in the dataset.</p></li><li><p><strong>on_epoch_end</strong> method: the indices of all the files in the dataset are created using np.arange(len(self.files)). If shuffle=True, the indices are then shuffled using np.random.shuffle(self.indexes). This ensures that during each epoch, the data is presented in a different order, which can help prevent the model from overfitting to any specific patterns in the data.</p></li><li><p>crop method: crop the data and as well as apply Gaussian smoothing to reduce noise and Laplacian filter to sharpen the image.</p></li></ul><p>Moreover, we have applied data augmentation technique and made sure masks have <strong>int type</strong> so we will have 0 and 1 in the mask after one-hot encoding. Additionally, the ultimate dimensions of both the image and mask are (1, 144, 224, 224, 4), with 1 representing the batch size, 144 signifying the depth, and 224 denoting the height and width respectively. The value 4 denotes the number of channels or classes, depending on whether the data pertains to the image or the mask.</p><p>To simplify the process, we have separated the training and validation data into distinct folders.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>train_dir = <span style=font-style:italic>&#34;/path_to_train_data/train&#34;</span>
</span></span><span style=display:flex><span>val_dir = <span style=font-style:italic>&#34;/path_to_val_data/validation&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>batch_size = 1
</span></span><span style=display:flex><span>shuffle = <span style=font-weight:700>True</span>
</span></span></code></pre></div><p>The next step involves creating two instances of a custom DataGenerator class, one for training data and another for validation data. The DataGenerator class generates batches of data for training or validation purposes, with optional data augmentation and shuffling.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>training_generator = DataGenerator(data_dir= train_dir, batch_size = batch_size, shuffle = <span style=font-weight:700>True</span>, augment = <span style=font-weight:700>True</span>)
</span></span><span style=display:flex><span>validation_generator = DataGenerator(data_dir= val_dir, batch_size = batch_size, shuffle = <span style=font-weight:700>True</span>, augment = <span style=font-weight:700>True</span>)
</span></span></code></pre></div><p>Now we can start the training;</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>checkpoint_filepath = <span style=font-style:italic>&#39;path_to_the_checkpoint/where_you_have_to_store/&#39;</span>
</span></span><span style=display:flex><span>checkpoint = tf.keras.callbacks.ModelCheckpoint(
</span></span><span style=display:flex><span>    filepath=checkpoint_filepath,
</span></span><span style=display:flex><span>    monitor=<span style=font-style:italic>&#39;loss&#39;</span>,
</span></span><span style=display:flex><span>    save_best_only = <span style=font-weight:700>True</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>history = model.fit(training_generator,epochs=4,
</span></span><span style=display:flex><span>                    validation_data=validation_generator,
</span></span><span style=display:flex><span>                    use_multiprocessing=<span style=font-weight:700>True</span>,callbacks=[checkpoint],
</span></span><span style=display:flex><span>                    workers=6, verbose=1) 
</span></span></code></pre></div><p>After 4 epoch, the model starts to converge and following is the result;</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>loss: 0.0469 - accuracy: 0.9876 - dice_coeff: 0.9788 - iou: 0.9588 - mean_iou: 0.9023 - val_loss: 0.0417 - val_accuracy: 0.9879 - val_dice_coeff: 0.9810 - val_iou: 0.9629 - val_mean_iou: 0.9194
</span></span></code></pre></div><p>Save the model;</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>model.save(<span style=font-style:italic>&#34;/path_to_save/model2.h5&#34;</span>)
</span></span></code></pre></div><p>Let us see how our model perform on the validation data;</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>nextbatch = next(iter(validation_generator))
</span></span><span style=display:flex><span>X_, y_ = nextbatch
</span></span></code></pre></div><p>Load the saved model</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>model_path = <span style=font-style:italic>&#34;path_to_the_model/model2.h5&#34;</span>
</span></span></code></pre></div><p>Define the custom objects;</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>custom_objects = {<span style=font-style:italic>&#39;loss&#39;</span>: <span style=font-style:italic>&#34;categorical_crossentropy&#34;</span>, <span style=font-style:italic>&#39;dice_coeff&#39;</span>:dice_coeff, <span style=font-style:italic>&#39;iou&#39;</span>: iou, <span style=font-style:italic>&#39;mean_iou&#39;</span>: mean_iou}
</span></span></code></pre></div><p>If you train with the custom functions, you have load with the custom function.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>modelbrats = load_model(model_path, custom_objects=custom_objects)
</span></span></code></pre></div><p>Load the saved weights</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>modelbrats.load_weights(model_path)
</span></span></code></pre></div><p>Finally, use the loaded weights to make predictions;</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>y_pred = modelbrats.predict(X_)
</span></span></code></pre></div><p>Let us visualize for an arbitary slice;</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>slice = 75
</span></span><span style=display:flex><span>y_pred_1 = y_pred[0, :, :, :, 0][slice]
</span></span><span style=display:flex><span>y_1 = y_[0, :, :, :, 0][slice]
</span></span><span style=display:flex><span>X_1 = X_[0, :, :, :, 0][slice]
</span></span></code></pre></div><p>If we check the <strong>np.unique(y_pred_1)</strong>, we get the following result.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>array([0.01908756, 0.01910349, 0.01917317, ..., 0.9999997 , 0.9999998 ,
</span></span><span style=display:flex><span>       0.99999994], dtype=float32)
</span></span></code></pre></div><p>We can notice the predicted pixel values are not 0 or 1. So let us convert them to intergers using <strong>np.around(y_pred_1)</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>y_pred_1 = np.around(y_pred_1)
</span></span></code></pre></div><p>Let us visualize ground truth, prediction and the brain for few slices.. Following is the code;</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=font-weight:700>import</span> <span style=font-weight:700>matplotlib.pyplot</span> <span style=font-weight:700>as</span> <span style=font-weight:700>plt</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>start_index = 70
</span></span><span style=display:flex><span>end_index = 74
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig, axs = plt.subplots((end_index - start_index + 1), 3, figsize=(15, 30))
</span></span><span style=display:flex><span>fig.subplots_adjust(wspace=0.3, hspace=0.3)
</span></span><span style=display:flex><span>fig.patch.set_facecolor(<span style=font-style:italic>&#39;white&#39;</span>)  <span style=font-style:italic># set the color of the coordinate axis to white</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>for</span> i <span style=font-weight:700>in</span> range(start_index, end_index+1):
</span></span><span style=display:flex><span>    axs[i-start_index, 0].imshow(np.round(y_pred[0,i, :, :, 0]), cmap=<span style=font-style:italic>&#39;bone&#39;</span>)
</span></span><span style=display:flex><span>    axs[i-start_index, 0].set_title(<span style=font-style:italic>&#39;y_pred </span><span style=font-weight:700;font-style:italic>{}</span><span style=font-style:italic>_Predicted&#39;</span>.format(i), color=<span style=font-style:italic>&#39;blue&#39;</span>)
</span></span><span style=display:flex><span>    axs[i-start_index, 1].imshow(y_[0,i, :, :, 0], cmap=<span style=font-style:italic>&#39;bone&#39;</span>)
</span></span><span style=display:flex><span>    axs[i-start_index, 1].set_title(<span style=font-style:italic>&#39;y_ </span><span style=font-weight:700;font-style:italic>{}</span><span style=font-style:italic>_Ground Truth&#39;</span>.format(i), color=<span style=font-style:italic>&#39;blue&#39;</span>)
</span></span><span style=display:flex><span>    axs[i-start_index, 2].imshow(X_[0,i, :, :, 0], cmap=<span style=font-style:italic>&#39;bone&#39;</span>)
</span></span><span style=display:flex><span>    axs[i-start_index, 2].set_title(<span style=font-style:italic>&#39;X_ </span><span style=font-weight:700;font-style:italic>{}</span><span style=font-style:italic>_Brain&#39;</span>.format(i), color=<span style=font-style:italic>&#39;blue&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt.show()
</span></span></code></pre></div><p>The figure below shows the predicted mask along with the ground truth.</p><figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/tumorspred.png></figure><p><strong>Conclusion</strong></p><p>3D UNet is better than 2D UNet in certain scenarios because it can capture more spatial information and provide better segmentation accuracy. 2D UNet works well for segmentation tasks on 2D images. However, when it comes to volumetric data such as medical images of organs, tissues, and lesions, the 2D U-Net may not perform as well because it cannot capture the spatial context of the data in the z-axis (depth). The 3D UNet is designed to handle volumetric data and captures spatial information in all three dimensions (x, y, and z). It can capture the context of the 3D image by taking into account the neighboring slices in the z-axis. As a result, it can produce more accurate segmentations than the 2D U-Net in volumetric data.</p></article></section></div><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></main><script src=https://AIThoughtLab.github.io/ThinkingAI/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>