<!doctype html><html lang=en><head><title>Building a Segmentation Model: Creating a UNet architecture from ResNet50 as encoder Â· AI for HUMANITY</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Mohamed ABDUL GAFOOR"><meta name=description content="In this post, I will be discussing a process of creating a UNet architecture where the encoder is sourced from the Resnet50. This technique is highly valuable, particularly when working with limited data, as it enables us to leverage the benefits of a pre-trained Resnet50 network. The UNet is an incredibly popular deep learning architecture for segmentation tasks and has become a go-to solution for many practitioners in this field. If you are interested in learning more about the original 2D UNet paper, I highly recommend referring to the source material."><meta name=keywords content="blog,developer,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="Building a Segmentation Model: Creating a UNet architecture from ResNet50 as encoder"><meta name=twitter:description content="In this post, I will be discussing a process of creating a UNet architecture where the encoder is sourced from the Resnet50. This technique is highly valuable, particularly when working with limited data, as it enables us to leverage the benefits of a pre-trained Resnet50 network. The UNet is an incredibly popular deep learning architecture for segmentation tasks and has become a go-to solution for many practitioners in this field. If you are interested in learning more about the original 2D UNet paper, I highly recommend referring to the source material."><meta property="og:title" content="Building a Segmentation Model: Creating a UNet architecture from ResNet50 as encoder"><meta property="og:description" content="In this post, I will be discussing a process of creating a UNet architecture where the encoder is sourced from the Resnet50. This technique is highly valuable, particularly when working with limited data, as it enables us to leverage the benefits of a pre-trained Resnet50 network. The UNet is an incredibly popular deep learning architecture for segmentation tasks and has become a go-to solution for many practitioners in this field. If you are interested in learning more about the original 2D UNet paper, I highly recommend referring to the source material."><meta property="og:type" content="article"><meta property="og:url" content="https://AIThoughtLab.github.io/ThinkingAI/projects/restnet/"><meta property="article:section" content="projects"><meta property="article:published_time" content="2023-04-05T00:00:00+00:00"><meta property="article:modified_time" content="2023-04-05T00:00:00+00:00"><link rel=canonical href=https://AIThoughtLab.github.io/ThinkingAI/projects/restnet/><link rel=preload href="https://AIThoughtLab.github.io/ThinkingAI/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=https://AIThoughtLab.github.io/ThinkingAI/css/coder.min.36f76aaf39a14ecf5c3a3c6250dcaf06c238b3d8365d17d646f95cb1874e852b.css integrity="sha256-NvdqrzmhTs9cOjxiUNyvBsI4s9g2XRfWRvlcsYdOhSs=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://AIThoughtLab.github.io/ThinkingAI/css/coder-dark.min.593028e7f7ac55c003b79c230d1cd411bb4ca53b31556c3abb7f027170e646e9.css integrity="sha256-WTAo5/esVcADt5wjDRzUEbtMpTsxVWw6u38CcXDmRuk=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=https://AIThoughtLab.github.io/ThinkingAI/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://AIThoughtLab.github.io/ThinkingAI/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=https://AIThoughtLab.github.io/ThinkingAI/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=https://AIThoughtLab.github.io/ThinkingAI/images/apple-touch-icon.png><link rel=manifest href=https://AIThoughtLab.github.io/ThinkingAI/site.webmanifest><link rel=mask-icon href=https://AIThoughtLab.github.io/ThinkingAI/images/safari-pinned-tab.svg color=#5bbad5><meta name=generator content="Hugo 0.111.3"></head><body class="preload-transitions colorscheme-dark"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://AIThoughtLab.github.io/ThinkingAI/>AI for HUMANITY</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/about/>About</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/physics/>PINN</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/innovations/>Innovations</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/personal/>Personal</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container page"><article><header><h1 class=title><a class=title-link href=https://AIThoughtLab.github.io/ThinkingAI/projects/restnet/>Building a Segmentation Model: Creating a UNet architecture from ResNet50 as encoder</a></h1></header><p>In this post, I will be discussing a process of creating a UNet architecture where the encoder is sourced from the <strong>Resnet50</strong>. This technique is highly valuable, particularly when working with limited data, as it enables us to leverage the benefits of a pre-trained Resnet50 network. The UNet is an incredibly popular deep learning architecture for segmentation tasks and has become a go-to solution for many practitioners in this field. If you are interested in learning more about the original 2D UNet paper, I highly recommend referring to the source <a href=https://arxiv.org/abs/1505.04597>material.</a></p><p>In the 2D UNet architecture, there are two main components: an encoder and a decoder network, which are linked by a bottleneck layer. The encoder network extracts features at multiple scales by using convolutional and max pooling layers to down-sample the input volume. On the other hand, the decoder network up-samples the feature maps to create a segmentation map. The bottleneck layer serves as the connection between the encoder and decoder networks, and it retains the spatial information of the input volume. By using this architecture, the 2D UNet can effectively segment images while preserving the spatial relationship between different regions of the image.</p><p>To accomplish this task, we will be utilizing a database specifically designed for hand gesture recognition. The dataset, which can be downloaded from the following link <a href=https://sun.aei.polsl.pl//~mkawulok/gestures/>here</a>, pertains to the HGR1 type of data. This dataset includes both original images and skin masks, and consists of 899 images from 12 individuals performing 25 unique gestures. The image dimensions in the dataset vary, ranging from 174x131 up to 640x480 pixels. The backgrounds in the images are uncontrolled, as are the lighting conditions. The masks have 4 channels, the last one is <strong>alpha</strong> channel. We can ignore the alpha channel safely. This is because the alpha channel encodes the transparency of the image and is not relevant for image segmentation.
Masks have 0 or 255 values and all three channels are identical, meaning, they have been duplicated across all three channels. In this case, we can treat the problem with only one channel and convert the masks to grayscale.</p><p>As a first step, import the necessary libraries;</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=font-weight:700>import</span> <span style=font-weight:700>numpy</span> <span style=font-weight:700>as</span> <span style=font-weight:700>np</span>
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>tensorflow.keras.preprocessing.image</span> <span style=font-weight:700>import</span> ImageDataGenerator
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>sklearn.model_selection</span> <span style=font-weight:700>import</span> train_test_split
</span></span><span style=display:flex><span><span style=font-weight:700>import</span> <span style=font-weight:700>cv2</span>
</span></span><span style=display:flex><span><span style=font-weight:700>import</span> <span style=font-weight:700>matplotlib.pyplot</span> <span style=font-weight:700>as</span> <span style=font-weight:700>plt</span>
</span></span><span style=display:flex><span><span style=font-weight:700>import</span> <span style=font-weight:700>numpy</span> <span style=font-weight:700>as</span> <span style=font-weight:700>np</span>
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>skimage.measure</span> <span style=font-weight:700>import</span> regionprops
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>tensorflow.keras.layers</span> <span style=font-weight:700>import</span> Input, Conv2D, MaxPooling2D, concatenate, UpSampling2D, LeakyReLU, BatchNormalization
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>tensorflow.keras.models</span> <span style=font-weight:700>import</span> Model
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>tensorflow.keras.applications</span> <span style=font-weight:700>import</span> ResNet50
</span></span><span style=display:flex><span><span style=font-weight:700>import</span> <span style=font-weight:700>tensorflow</span> <span style=font-weight:700>as</span> <span style=font-weight:700>tf</span>
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>tensorflow.keras.models</span> <span style=font-weight:700>import</span> load_model
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>tensorflow.keras.utils</span> <span style=font-weight:700>import</span> plot_model
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>google.colab.patches</span> <span style=font-weight:700>import</span> cv2_imshow
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>keras.callbacks</span> <span style=font-weight:700>import</span> EarlyStopping
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>tensorflow.keras.regularizers</span> <span style=font-weight:700>import</span> l2
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>keras.utils</span> <span style=font-weight:700>import</span> to_categorical
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>tensorflow.keras</span> <span style=font-weight:700>import</span> layers
</span></span></code></pre></div><p>Set the path to the image and the mask folder.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>image_folder = <span style=font-style:italic>&#34;./original_images/&#34;</span>
</span></span><span style=display:flex><span>mask_folder = <span style=font-style:italic>&#34;./skin_masks/&#34;</span>
</span></span></code></pre></div><ul><li>We will create list of filenames for images and masks.</li><li>Split the filenames into training and testing sets.</li><li>Create the full paths to the training and testing images and masks</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>image_filenames = sorted(os.listdir(image_folder))
</span></span><span style=display:flex><span>mask_filenames = sorted(os.listdir(mask_folder))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-style:italic>#  Split the filenames</span>
</span></span><span style=display:flex><span>train_image_filenames, test_image_filenames, train_mask_filenames, test_mask_filenames = train_test_split(image_filenames, mask_filenames, test_size=0.2)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-style:italic>#Create the full paths</span>
</span></span><span style=display:flex><span>train_image_paths = [os.path.join(image_folder, filename) <span style=font-weight:700>for</span> filename <span style=font-weight:700>in</span> train_image_filenames]
</span></span><span style=display:flex><span>test_image_paths = [os.path.join(image_folder, filename) <span style=font-weight:700>for</span> filename <span style=font-weight:700>in</span> test_image_filenames]
</span></span><span style=display:flex><span>train_mask_paths = [os.path.join(mask_folder, filename) <span style=font-weight:700>for</span> filename <span style=font-weight:700>in</span> train_mask_filenames]
</span></span><span style=display:flex><span>test_mask_paths = [os.path.join(mask_folder, filename) <span style=font-weight:700>for</span> filename <span style=font-weight:700>in</span> test_mask_filenames]
</span></span></code></pre></div><p>We will display few images and the corresponding masks;</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=font-weight:700>def</span> plot_images_and_masks(image_paths, mask_paths):
</span></span><span style=display:flex><span>    fig, ax = plt.subplots(len(image_paths), 2, figsize=(5, 5))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=font-weight:700>for</span> i, (image_path, mask_path) <span style=font-weight:700>in</span> enumerate(zip(image_paths, mask_paths)):
</span></span><span style=display:flex><span>        <span style=font-style:italic># Load image and mask</span>
</span></span><span style=display:flex><span>        img = plt.imread(image_path)
</span></span><span style=display:flex><span>        img = normalize_array(img)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        mask = plt.imread(mask_path)
</span></span><span style=display:flex><span>        mask = normalize_array(mask)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic># Plot image and mask side by side</span>
</span></span><span style=display:flex><span>        ax[i, 0].imshow(img)
</span></span><span style=display:flex><span>        ax[i, 1].imshow(mask, cmap=<span style=font-style:italic>&#39;gray&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic># Set title</span>
</span></span><span style=display:flex><span>        ax[i, 0].set_title(<span style=font-style:italic>f</span><span style=font-style:italic>&#34;Image </span><span style=font-weight:700;font-style:italic>{</span>i+1<span style=font-weight:700;font-style:italic>}</span><span style=font-style:italic>&#34;</span>)
</span></span><span style=display:flex><span>        ax[i, 1].set_title(<span style=font-style:italic>f</span><span style=font-style:italic>&#34;Mask </span><span style=font-weight:700;font-style:italic>{</span>i+1<span style=font-weight:700;font-style:italic>}</span><span style=font-style:italic>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic># Remove axis ticks</span>
</span></span><span style=display:flex><span>        ax[i, 0].axis(<span style=font-style:italic>&#39;off&#39;</span>)
</span></span><span style=display:flex><span>        ax[i, 1].axis(<span style=font-style:italic>&#39;off&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    plt.tight_layout()
</span></span><span style=display:flex><span>    plt.show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plot_images_and_masks(train_image_paths[0:5], train_mask_paths[0:5])
</span></span></code></pre></div><p>Following is the visualization of 5 images and masks.</p><figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/handsmasks.png></figure><p><strong>Data Generator</strong></p><p>When working with large amounts of data, it is not feasible to load all the data into memory at once, so we need a way to load the data in batches. This is where a data generator comes in. A data generator is a Python generator that yields batches of data on-the-fly during training. It loads the data from disk or other data sources, performs data augmentation and preprocessing, and then passes the data to the model for training. This allows us to efficiently work with large datasets that cannot fit into memory.</p><p>In addition to providing a way to load large datasets, data generators are also useful for data augmentation. Data augmentation is a technique where we create new data from existing data by applying transformations such as rotation, flipping, and scaling. By generating new data from the existing data, we can increase the size of our dataset and improve the generalization of the model.</p><p>Following is the data generator class in our case;</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>keras.preprocessing.image</span> <span style=font-weight:700>import</span> ImageDataGenerator
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>class</span> <span style=font-weight:700>DataGenerator</span>(tf.keras.utils.Sequence):
</span></span><span style=display:flex><span>    <span style=font-weight:700>def</span> __init__(self, image_paths, mask_paths, image_size=(224,224), batch_size=16, img_channels=3, mask_channels = 1, shuffle=<span style=font-weight:700>True</span>, augmentation=<span style=font-weight:700>True</span>):
</span></span><span style=display:flex><span>        self.image_paths = image_paths
</span></span><span style=display:flex><span>        self.mask_paths = mask_paths
</span></span><span style=display:flex><span>        self.batch_size = batch_size
</span></span><span style=display:flex><span>        self.shuffle = shuffle
</span></span><span style=display:flex><span>        self.indexes = np.arange(len(self.image_paths))
</span></span><span style=display:flex><span>        self.on_epoch_end()
</span></span><span style=display:flex><span>        self.img_channels = img_channels
</span></span><span style=display:flex><span>        self.image_size = image_size
</span></span><span style=display:flex><span>        self.mask_channels = mask_channels
</span></span><span style=display:flex><span>        self.augmentation = augmentation
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=font-weight:700>def</span> __len__(self):
</span></span><span style=display:flex><span>        <span style=font-weight:700>return</span> int(np.ceil(len(self.image_paths) / self.batch_size))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=font-weight:700>def</span> __getitem__(self, index):
</span></span><span style=display:flex><span>        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]
</span></span><span style=display:flex><span>        image_paths = [self.image_paths[k] <span style=font-weight:700>for</span> k <span style=font-weight:700>in</span> indexes]
</span></span><span style=display:flex><span>        mask_paths = [self.mask_paths[k] <span style=font-weight:700>for</span> k <span style=font-weight:700>in</span> indexes]
</span></span><span style=display:flex><span>        X, y = self.__data_generation(image_paths, mask_paths)
</span></span><span style=display:flex><span>        <span style=font-weight:700>return</span> X, y
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=font-weight:700>def</span> on_epoch_end(self):
</span></span><span style=display:flex><span>        <span style=font-weight:700>if</span> self.shuffle:
</span></span><span style=display:flex><span>            np.random.shuffle(self.indexes)
</span></span><span style=display:flex><span>    <span style=font-style:italic># https://lindevs.com/apply-gamma-correction-to-an-image-using-opencv</span>
</span></span><span style=display:flex><span>    <span style=font-weight:700>def</span> gammaCorrection(self, src, gamma):
</span></span><span style=display:flex><span>        invGamma = 1 / gamma
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        table = [((i / 255) ** invGamma) * 255 <span style=font-weight:700>for</span> i <span style=font-weight:700>in</span> range(256)]
</span></span><span style=display:flex><span>        table = np.array(table, np.uint8)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-weight:700>return</span> cv2.LUT(src, table)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=font-style:italic>&#34;&#34;&#34;Correct normalization is important..&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=font-weight:700>def</span> normalize_array(self, img):
</span></span><span style=display:flex><span>        min_val = np.min(img)
</span></span><span style=display:flex><span>        max_val = np.max(img)
</span></span><span style=display:flex><span>        normalized_img = (img - min_val) / (max_val - min_val)
</span></span><span style=display:flex><span>        <span style=font-weight:700>return</span> normalized_img
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=font-weight:700>def</span> __data_generation(self, image_paths, mask_paths):
</span></span><span style=display:flex><span>        <span style=font-style:italic># Load images and masks</span>
</span></span><span style=display:flex><span>        X = np.empty((self.batch_size, *self.image_size, self.img_channels),  dtype=np.float32)
</span></span><span style=display:flex><span>        y = np.empty((self.batch_size, *self.image_size, self.mask_channels))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-weight:700>for</span> i, (image_path, mask_path) <span style=font-weight:700>in</span> enumerate(zip(image_paths, mask_paths)):
</span></span><span style=display:flex><span>            img = plt.imread(image_path)
</span></span><span style=display:flex><span>            img = cv2.resize(img, self.image_size)
</span></span><span style=display:flex><span>            img = self.normalize_array(img)
</span></span><span style=display:flex><span>            X[i,] = img
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            mask = plt.imread(mask_path)
</span></span><span style=display:flex><span>            mask = mask[:, :, :3] <span style=font-style:italic># Omit Alpha channel.. </span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=font-style:italic># Set background pixels to 0 and object pixels to 1</span>
</span></span><span style=display:flex><span>            mask[mask == 0] = 1
</span></span><span style=display:flex><span>            mask[mask == 255] = 0
</span></span><span style=display:flex><span>            <span style=font-style:italic># print(mask.shape)</span>
</span></span><span style=display:flex><span>            mask = np.mean(mask, axis=2)
</span></span><span style=display:flex><span>            <span style=font-style:italic># print(mask.shape)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            mask = cv2.resize(mask, self.image_size)
</span></span><span style=display:flex><span>            mask = np.reshape(mask, (224, 224, 1))
</span></span><span style=display:flex><span>            mask = mask.astype(<span style=font-style:italic>&#39;int&#39;</span>) 
</span></span><span style=display:flex><span>            <span style=font-style:italic>#print(&#34;mask unique3 :&#34;, np.unique(mask))</span>
</span></span><span style=display:flex><span>            <span style=font-style:italic>#mask = mask.astype(np.uint8)</span>
</span></span><span style=display:flex><span>            y[i,] = mask
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>         <span style=font-style:italic># Data augmentation</span>
</span></span><span style=display:flex><span>        <span style=font-weight:700>if</span> self.augmentation:
</span></span><span style=display:flex><span>          seed = np.random.randint(1, 100)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>          data_gen_args = dict(horizontal_flip=<span style=font-weight:700>True</span>,
</span></span><span style=display:flex><span>                              vertical_flip=<span style=font-weight:700>True</span>,
</span></span><span style=display:flex><span>                               )
</span></span><span style=display:flex><span>          
</span></span><span style=display:flex><span>          image_data_generator = ImageDataGenerator(**data_gen_args)
</span></span><span style=display:flex><span>          mask_data_generator = ImageDataGenerator(**data_gen_args)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>          image_data_generator.fit(X, augment=<span style=font-weight:700>True</span>, seed=seed)
</span></span><span style=display:flex><span>          mask_data_generator.fit(y, augment=<span style=font-weight:700>True</span>, seed=seed)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>          X = image_data_generator.flow(X, batch_size=self.batch_size, shuffle=<span style=font-weight:700>False</span>, seed=seed)
</span></span><span style=display:flex><span>          y = mask_data_generator.flow(y, batch_size=self.batch_size, shuffle=<span style=font-weight:700>False</span>, seed=seed)
</span></span><span style=display:flex><span>          
</span></span><span style=display:flex><span>          X, y = next(zip(X, y))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        y = y.astype(int)
</span></span><span style=display:flex><span>        X = np.round(X,3)  <span style=font-style:italic>#np.round(x,4) or X.astype(np.float32)</span>
</span></span><span style=display:flex><span>        <span style=font-weight:700>return</span> X, y
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-style:italic># Training generator</span>
</span></span><span style=display:flex><span>training_generator = DataGenerator(train_image_paths, train_mask_paths, shuffle=<span style=font-weight:700>True</span>, augmentation=<span style=font-weight:700>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-style:italic># Validation generator</span>
</span></span><span style=display:flex><span>validation_generator = DataGenerator(test_image_paths, test_mask_paths, shuffle=<span style=font-weight:700>True</span>, augmentation=<span style=font-weight:700>True</span>)
</span></span></code></pre></div><p><strong>Data augmentation</strong></p><p>Data augmentation is a technique used in deep learning to artificially increase the size of the training dataset by creating new training samples through transformations of the original data. For example, if you are working with images, you can apply various image transformation techniques like cropping, flipping, rotating, zooming, or adding noise to create new images from the original dataset. It helps to improve generalization, mitigate overfitting issues, and reduce the need for more data.</p><p>In the subsequent steps, we begin by loading <strong>ResNet50</strong> as the base model and set the weights to &lsquo;imagenet&rsquo;. To accomplish our task, we trim ResNet50 by obtaining the output of the fourth convolutional block. We then proceed to finetune the model by freezing the layers up to &lsquo;conv4_block4_2_conv&rsquo;, while the remaining layers will be trained.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=font-style:italic># Load ResNet50 as the base model</span>
</span></span><span style=display:flex><span>model = ResNet50(include_top=<span style=font-weight:700>False</span>, input_shape=(224, 224, 3), weights=<span style=font-style:italic>&#39;imagenet&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-style:italic># Get the output of the fourth convolutional block</span>
</span></span><span style=display:flex><span>conv4_block6_out = model.get_layer(<span style=font-style:italic>&#39;conv4_block6_out&#39;</span>).output
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-style:italic># Create a new model that outputs the feature maps of the fourth convolutional block</span>
</span></span><span style=display:flex><span>base_model = tf.keras.models.Model(inputs=model.input, outputs=conv4_block6_out)
</span></span><span style=display:flex><span>base_model.trainable = <span style=font-weight:700>True</span>
</span></span><span style=display:flex><span>set_trainable = <span style=font-weight:700>False</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-style:italic># Freeze the layers of the base model and fine tune. </span>
</span></span><span style=display:flex><span><span style=font-weight:700>for</span> layer <span style=font-weight:700>in</span> base_model.layers:
</span></span><span style=display:flex><span>    <span style=font-weight:700>if</span> layer.name == <span style=font-style:italic>&#39;conv4_block4_2_conv&#39;</span>:
</span></span><span style=display:flex><span>      set_trainable == <span style=font-weight:700>True</span>
</span></span><span style=display:flex><span>    <span style=font-weight:700>if</span> set_trainable:
</span></span><span style=display:flex><span>      layer.trainable = <span style=font-weight:700>True</span>
</span></span><span style=display:flex><span>    <span style=font-weight:700>else</span>:
</span></span><span style=display:flex><span>      layer.trainable = <span style=font-weight:700>False</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>base_model.summary()
</span></span></code></pre></div><p>We have determined the specific layers from ResNet50 to be utilized during the deconvolution process. These layers can be concatenated during this step.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>conv3_block3_out = base_model.get_layer(<span style=font-style:italic>&#39;conv3_block3_2_conv&#39;</span>).output <span style=font-style:italic># conv3_block3_out</span>
</span></span><span style=display:flex><span>conv2_block3_out = base_model.get_layer(<span style=font-style:italic>&#39;conv2_block3_3_conv&#39;</span>).output <span style=font-style:italic># conv2_block3_out</span>
</span></span><span style=display:flex><span>conv1_conv = base_model.get_layer(<span style=font-style:italic>&#39;conv1_conv&#39;</span>).output
</span></span></code></pre></div><p>In the U-Net architecture, a bottleneck layer is introduced between the contracting and expanding paths. This bottleneck layer is essentially the bottleneck of the network where the number of feature maps is the smallest. It serves the purpose of bridging the contracting path to the expanding path while maintaining the high-resolution features.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=font-style:italic># Bottle neck</span>
</span></span><span style=display:flex><span>bn = base_model.get_layer(<span style=font-style:italic>&#39;conv4_block6_3_conv&#39;</span>).output <span style=font-style:italic># conv4_block6_out</span>
</span></span></code></pre></div><p>Then up convolution is defined as follow. Here we also use Batch Normalization. The primary purpose of batch normalization is to address the internal covariate shift problem. Internal covariate shift occurs when the distribution of the input to a layer changes as the parameters of the previous layers are updated during training. This makes it difficult for the subsequent layers to learn and adapt to the new distribution of inputs.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=font-style:italic># Apply up-convolution</span>
</span></span><span style=display:flex><span>up1 = tf.keras.layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding=<span style=font-style:italic>&#39;same&#39;</span>)(bn)
</span></span><span style=display:flex><span>up1 = tf.keras.layers.concatenate([up1, conv3_block3_out], axis=-1)
</span></span><span style=display:flex><span>conv5 = tf.keras.layers.Conv2D(512, (3, 3), padding=<span style=font-style:italic>&#39;same&#39;</span>)(up1)
</span></span><span style=display:flex><span>conv5 = tf.keras.layers.Dropout(0.25)(conv5)
</span></span><span style=display:flex><span>conv5 = tf.keras.layers.BatchNormalization()(conv5)
</span></span><span style=display:flex><span>conv5 = tf.keras.layers.ReLU()(conv5)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>up2 = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding=<span style=font-style:italic>&#39;same&#39;</span>)(conv5)
</span></span><span style=display:flex><span>up2 = tf.keras.layers.concatenate([up2, conv2_block3_out], axis=-1)
</span></span><span style=display:flex><span>conv6 = tf.keras.layers.Conv2D(256, (3, 3), padding=<span style=font-style:italic>&#39;same&#39;</span>)(up2)
</span></span><span style=display:flex><span>conv6 = tf.keras.layers.Dropout(0.25)(conv6)
</span></span><span style=display:flex><span>conv6 = tf.keras.layers.BatchNormalization()(conv6)
</span></span><span style=display:flex><span>conv6 = tf.keras.layers.ReLU()(conv6)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>up3 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding=<span style=font-style:italic>&#39;same&#39;</span>)(conv6)
</span></span><span style=display:flex><span>up3 = tf.keras.layers.concatenate([up3, conv1_conv], axis=-1)
</span></span><span style=display:flex><span>conv7 = tf.keras.layers.Conv2D(64, (3, 3), padding=<span style=font-style:italic>&#39;same&#39;</span>)(up3)
</span></span><span style=display:flex><span>conv7 = tf.keras.layers.Dropout(0.25)(conv7)
</span></span><span style=display:flex><span>conv7 = tf.keras.layers.BatchNormalization()(conv7)
</span></span><span style=display:flex><span>conv7 = tf.keras.layers.ReLU()(conv7)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>conv7 = UpSampling2D(size=(2, 2))(conv7)
</span></span><span style=display:flex><span><span style=font-style:italic># Add a convolutional layer to reduce the number of channels to 3</span>
</span></span><span style=display:flex><span>output = Conv2D(1, (1, 1), activation=<span style=font-style:italic>&#39;sigmoid&#39;</span>)(conv7)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model = tf.keras.models.Model(inputs=base_model.input, outputs=output)
</span></span><span style=display:flex><span>model.summary()
</span></span></code></pre></div><p>Both dice coefficient and intersection over union (IoU) are evaluation metrics used in image segmentation tasks to measure the similarity between two sets of pixels. Dice coefficient measures the overlap between two masks, while IoU measures the intersection over the union of the two masks. Both metrics range from 0 to 1, with 1 indicating perfect overlap and 0 indicating no overlap.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=font-weight:700>def</span> dice_coeff(y_true, y_pred, smooth=1):
</span></span><span style=display:flex><span>    y_true = tf.cast(y_true, y_pred.dtype)
</span></span><span style=display:flex><span>    y_true_f = K.flatten(y_true)
</span></span><span style=display:flex><span>    y_pred_f = K.flatten(y_pred)
</span></span><span style=display:flex><span>    intersection = K.sum(y_true_f * y_pred_f)
</span></span><span style=display:flex><span>    <span style=font-weight:700>return</span> (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>def</span> dice_loss(y_true, y_pred, smooth=1):
</span></span><span style=display:flex><span>    dc = dice_coeff(y_true, y_pred, smooth=smooth)
</span></span><span style=display:flex><span>    loss = 1.0 - dc
</span></span><span style=display:flex><span>    <span style=font-weight:700>return</span> loss
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>def</span> iou(y_true, y_pred, smooth=1):
</span></span><span style=display:flex><span>    y_true = tf.cast(y_true, y_pred.dtype)
</span></span><span style=display:flex><span>    y_true_f = K.flatten(y_true)
</span></span><span style=display:flex><span>    y_pred_f = K.flatten(y_pred)
</span></span><span style=display:flex><span>    intersection = K.sum(y_true_f * y_pred_f)
</span></span><span style=display:flex><span>    union = K.sum(y_true_f) + K.sum(y_pred_f) - intersection
</span></span><span style=display:flex><span>    <span style=font-weight:700>return</span> (intersection + smooth) / (union + smooth)
</span></span></code></pre></div><p>Now time to compile the model. The purpose of setting clipnorm to a value is to prevent the gradients from becoming too large during training (sometimes we ended up with <strong>Nan</strong> value), which can cause the optimization algorithm to overshoot the minimum of the loss function and lead to poor convergence or divergence.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>learning_rate = 0.001
</span></span><span style=display:flex><span>optimizer = Adam(learning_rate=learning_rate, clipnorm=1)
</span></span><span style=display:flex><span>model.compile(optimizer = optimizer, loss= dice_loss, 
</span></span><span style=display:flex><span>              metrics = [<span style=font-style:italic>&#39;accuracy&#39;</span>,dice_coeff, iou])
</span></span><span style=display:flex><span>checkpoint_filepath = <span style=font-style:italic>&#39;/path_to_the_checpoint_file/&#39;</span>
</span></span><span style=display:flex><span>checkpoint = tf.keras.callbacks.ModelCheckpoint(
</span></span><span style=display:flex><span>    filepath=checkpoint_filepath,
</span></span><span style=display:flex><span>    monitor=<span style=font-style:italic>&#39;loss&#39;</span>,
</span></span><span style=display:flex><span>    save_best_only = <span style=font-weight:700>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>earlystop = EarlyStopping(patience=7, verbose=1)
</span></span><span style=display:flex><span>history = model.fit(training_generator,epochs=20,
</span></span><span style=display:flex><span>                    validation_data = validation_generator,
</span></span><span style=display:flex><span>                    use_multiprocessing=<span style=font-weight:700>True</span>,callbacks=[checkpoint, earlystop],
</span></span><span style=display:flex><span>                    workers=6, verbose=1)
</span></span></code></pre></div><p>Following figure shows training and validation loss after 20 epochs.<figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/lossfun.png></figure></p><p>Now let us save the model and test for unseen data;</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>model.save(<span style=font-style:italic>&#34;./model.h5&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-style:italic># Load the saved model</span>
</span></span><span style=display:flex><span>model_path = <span style=font-style:italic>&#34;./model.h5&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>custom_objects = {<span style=font-style:italic>&#39;dice_loss&#39;</span>:dice_loss, <span style=font-style:italic>&#39;dice_coeff&#39;</span>:dice_coeff, <span style=font-style:italic>&#39;iou&#39;</span>: iou}
</span></span><span style=display:flex><span>modelhand = load_model(model_path, custom_objects=custom_objects)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-style:italic># Load the saved weights</span>
</span></span><span style=display:flex><span>modelhand.load_weights(model_path)
</span></span></code></pre></div><p>Following are a few predicted segmentations for unseen data.<figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/unseen.png></figure></p><p><strong>Conclusion</strong></p><p>We have attempted to create a U-Net by utilizing the ResNet50 architecture along with its pre-trained &lsquo;imagenet&rsquo; weights. This approach is particularly useful since we can leverage the benefits of pre-trained networks to achieve better performance and efficiency in our model.</p></article></section></div><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></main><script src=https://AIThoughtLab.github.io/ThinkingAI/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>