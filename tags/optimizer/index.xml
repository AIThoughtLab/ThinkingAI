<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Optimizer on AI for HUMANITY</title><link>https://AIThoughtLab.github.io/ThinkingAI/tags/optimizer/</link><description>Recent content in Optimizer on AI for HUMANITY</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 20 Apr 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://AIThoughtLab.github.io/ThinkingAI/tags/optimizer/index.xml" rel="self" type="application/rss+xml"/><item><title>Gradient Decent, Stochastic Gradient Descent and Adam Optimization Algorithm</title><link>https://AIThoughtLab.github.io/ThinkingAI/posts/adam/</link><pubDate>Sat, 20 Apr 2019 00:00:00 +0000</pubDate><guid>https://AIThoughtLab.github.io/ThinkingAI/posts/adam/</guid><description>Optimization Algorithms
Before we start the Adaptive Moment Estimation (Adam) optimization algorithm, we first see what is Gradient Decent (GD) optimization and how it works and then discuss about Stochastic Gradient Descent (SGD) and finally Adam optimization.
Gradient Decent (GD)
GD is one of the most well known optimization algorithm available in machine learning and deep learning framework. Lets say we have a scalar cost function C which is continuous in the domain \(R_n\) (n-dimensional real vector space), takes the input vector \(x ( = x_1, x_2 &amp;hellip; x_n)\) of length \(n\) and we would like to find an optimal value of this cost function.</description></item></channel></rss>