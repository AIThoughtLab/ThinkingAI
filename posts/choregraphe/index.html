<!doctype html><html lang=en><head><title>Choregraphe Software for Pepper Robot · AI for HUMANITY</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Mohamed ABDUL GAFOOR"><meta name=description content="The purpose of this post is to build up a robot speaking system with body language and speech text and speech sound in Choregraphe. See the documentation here or download the software here.
The process is very straight forward using this software.
Step-1: Firstly we opened Choregraphe App and connected to the virtual robot &lsquo;NAO H21​&rsquo;. After that, we selected the &lsquo;Say&rsquo; node, ​&rsquo;Delay&rsquo; node and &lsquo;Play Sound&rsquo; ​node on the Box library."><meta name=keywords content="blog,developer,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="Choregraphe Software for Pepper Robot"><meta name=twitter:description content="The purpose of this post is to build up a robot speaking system with body language and speech text and speech sound in Choregraphe. See the documentation here or download the software here.
The process is very straight forward using this software.
Step-1: Firstly we opened Choregraphe App and connected to the virtual robot &lsquo;NAO H21​&rsquo;. After that, we selected the &lsquo;Say&rsquo; node, ​&rsquo;Delay&rsquo; node and &lsquo;Play Sound&rsquo; ​node on the Box library."><meta property="og:title" content="Choregraphe Software for Pepper Robot"><meta property="og:description" content="The purpose of this post is to build up a robot speaking system with body language and speech text and speech sound in Choregraphe. See the documentation here or download the software here.
The process is very straight forward using this software.
Step-1: Firstly we opened Choregraphe App and connected to the virtual robot &lsquo;NAO H21​&rsquo;. After that, we selected the &lsquo;Say&rsquo; node, ​&rsquo;Delay&rsquo; node and &lsquo;Play Sound&rsquo; ​node on the Box library."><meta property="og:type" content="article"><meta property="og:url" content="https://AIThoughtLab.github.io/ThinkingAI/posts/choregraphe/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-12-05T00:00:00+00:00"><meta property="article:modified_time" content="2020-12-05T00:00:00+00:00"><link rel=canonical href=https://AIThoughtLab.github.io/ThinkingAI/posts/choregraphe/><link rel=preload href="https://AIThoughtLab.github.io/ThinkingAI/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=https://AIThoughtLab.github.io/ThinkingAI/css/coder.min.36f76aaf39a14ecf5c3a3c6250dcaf06c238b3d8365d17d646f95cb1874e852b.css integrity="sha256-NvdqrzmhTs9cOjxiUNyvBsI4s9g2XRfWRvlcsYdOhSs=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://AIThoughtLab.github.io/ThinkingAI/css/coder-dark.min.593028e7f7ac55c003b79c230d1cd411bb4ca53b31556c3abb7f027170e646e9.css integrity="sha256-WTAo5/esVcADt5wjDRzUEbtMpTsxVWw6u38CcXDmRuk=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=https://AIThoughtLab.github.io/ThinkingAI/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://AIThoughtLab.github.io/ThinkingAI/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=https://AIThoughtLab.github.io/ThinkingAI/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=https://AIThoughtLab.github.io/ThinkingAI/images/apple-touch-icon.png><link rel=manifest href=https://AIThoughtLab.github.io/ThinkingAI/site.webmanifest><link rel=mask-icon href=https://AIThoughtLab.github.io/ThinkingAI/images/safari-pinned-tab.svg color=#5bbad5><meta name=generator content="Hugo 0.111.2"></head><body class="preload-transitions colorscheme-dark"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://AIThoughtLab.github.io/ThinkingAI/>AI for HUMANITY</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/about/>About</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/innovations/>Innovations</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/personal/>Personal</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://AIThoughtLab.github.io/ThinkingAI/posts/choregraphe/>Choregraphe Software for Pepper Robot</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2020-12-05T00:00:00Z>December 5, 2020</time></span>
<span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
3-minute read</span></div><div class=categories><i class="fa fa-folder" aria-hidden=true></i>
<a href=https://AIThoughtLab.github.io/ThinkingAI/categories/artificial-intelligence/>Artificial Intelligence</a>
<span class=separator>•</span>
<a href=https://AIThoughtLab.github.io/ThinkingAI/categories/social-robotics/>Social Robotics</a></div><div class=tags><i class="fa fa-tag" aria-hidden=true></i>
<span class=tag><a href=https://AIThoughtLab.github.io/ThinkingAI/tags/social-robotics/>Social Robotics</a></span></div></div></header><div class=post-content><p>The purpose of this post is to build up a robot speaking system with body language and speech text and speech sound in Choregraphe. See the <a href=http://doc.aldebaran.com/2-4/software/choregraphe/index.html>documentation here</a> or <a href=https://www.robotlab.com/choregraphe-download-page-for-pepper-robot>download</a> the software here.</p><p>The process is very straight forward using this software.</p><p>Step-1:
Firstly we opened Choregraphe App and connected to the virtual robot &lsquo;NAO H21​&rsquo;. After
that, we selected the &lsquo;Say&rsquo; node, ​&rsquo;Delay&rsquo; node and &lsquo;Play Sound&rsquo; ​node on the Box library. In
the ​&rsquo;Say&rsquo; node, we put in the words that Pepper will speak. (Here we test with &lsquo;NAO H21&rsquo;
but finally we changed it to ​&rsquo;Pepper&rsquo;​). Then go to ​Connection ​and ​Connect to virtual robot
in menu.</p><figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/step1.png></figure><p>Add the following nodes in order to create ​text​, ​delay ​and ​sounds ​in the virtual object. The
nodes can be added easy by searching the key words in the search windows (show filters).
The following screenshot shows how this can be accomplished.</p><figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/say.png></figure><p>If we want to add the text to the &lsquo;say&rsquo; node, we can simply right click and ​&rsquo;set parameters&rsquo; of say. The following screenshot shows how this can be achieved.</p><figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/pep.png></figure><p>Step 2:
In order to listen to Pepper speaking, we added several &lsquo;Play Sound&rsquo; nodes in the Box
Library and put the .wav file as parameters. Here we used &rsquo;text2speech&rsquo; to generate sound
files. We only need to type in the words to be converted as voice and download the
generated file. The link of &rsquo;text2speech&rsquo; is provided in reference.</p><figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/text.png></figure><p>The following figure shows how an audio file can be added to the Play Sound node.</p><figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/audio.png></figure><p>Step 3:
We created as well the &lsquo;Delay&rsquo; node in order to have the text and sound synchronized and
to imitate the natural way of speaking. We tuned the time in each Delay node to achieve it.
Right click on the Delay node and set parameters. Where we can set the time of delay after
each text/audio.</p><figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/delay.png></figure><p>Step 4:
At this step, we would create animations that are specified in the provided json file. We
created a python script by creating a new box with right click. The below image shows the
procedure to create a Python script.</p><figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/py.png></figure><p>In the <strong>init</strong> method, add the following lines. This method is called when an object is
created from MyClass. This method helps the class to initialize the attributes of the class.
First we have to create a session using qi.Session(), then add the tcp IP and port. The
default IP is &ldquo;127.0.0.1&rdquo;.</p><figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/iniy.png></figure><p>The port information can be obtained by go to Edit → Preference → Virtual Robot.<figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/port.png></figure></p><p>Now in the def onInput_onStart(self) method, add the following line to create the
necessary animation. We must set the path to the json file and load the json file and store it
into the robotpose. If you want to run this project in your local machine, don’t forget to
change the path of json file in the code below. Make sure to set the correct port information.</p><figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/oninput.png></figure><p>The following screenshot shows the final implementation of the Choregraphe project.</p><figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/chro.png></figure><p>Visit the <a href=https://github.com/AIThoughtLab/Choregraphe>Github page</a> to see the implementation.</p><p>Reference:</p><ol><li>Text 2 Speech</li><li>Choregraphe application: NAO6 Downloads - Linux | SoftBank Robotics Developer Center</li><li>Joint control API — Aldebaran 2.5.11.14a documentation</li><li><a href=http://doc.aldebaran.com/2-5/naoqi/motion/control-joint-api.html#ALMotionProxy::angleInterpolation__AL::ALValueCR.AL::ALValueCR.AL::ALValueCR.bCR>ALMotionProxy</a></li></ol></div><footer></footer></article></section></div><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></main><script src=https://AIThoughtLab.github.io/ThinkingAI/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>