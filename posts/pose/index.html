<!doctype html><html lang=en><head><title>Human 3D Pose Reconstruction · AI for HUMANITY</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Mohamed ABDUL GAFOOR"><meta name=description content="Human 3D Pose Reconstruction
This blog focuses on performing 3D reconstruction of human poses using multiple views captured by a calibrated system. To achieve this, a deep neural network called OpenPose is used to estimate the location of the joints in the human body. The OpenPose model has been trained to recognize 25 different joints in the human body, such as the head, shoulders, elbows, hips, knees, and ankles. These joints are critical for determining the posture and movement of a person."><meta name=keywords content="blog,developer,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="Human 3D Pose Reconstruction"><meta name=twitter:description content="Human 3D Pose Reconstruction
This blog focuses on performing 3D reconstruction of human poses using multiple views captured by a calibrated system. To achieve this, a deep neural network called OpenPose is used to estimate the location of the joints in the human body. The OpenPose model has been trained to recognize 25 different joints in the human body, such as the head, shoulders, elbows, hips, knees, and ankles. These joints are critical for determining the posture and movement of a person."><meta property="og:title" content="Human 3D Pose Reconstruction"><meta property="og:description" content="Human 3D Pose Reconstruction
This blog focuses on performing 3D reconstruction of human poses using multiple views captured by a calibrated system. To achieve this, a deep neural network called OpenPose is used to estimate the location of the joints in the human body. The OpenPose model has been trained to recognize 25 different joints in the human body, such as the head, shoulders, elbows, hips, knees, and ankles. These joints are critical for determining the posture and movement of a person."><meta property="og:type" content="article"><meta property="og:url" content="https://AIThoughtLab.github.io/ThinkingAI/posts/pose/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-12-26T00:00:00+00:00"><meta property="article:modified_time" content="2020-12-26T00:00:00+00:00"><link rel=canonical href=https://AIThoughtLab.github.io/ThinkingAI/posts/pose/><link rel=preload href="https://AIThoughtLab.github.io/ThinkingAI/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=https://AIThoughtLab.github.io/ThinkingAI/css/coder.min.36f76aaf39a14ecf5c3a3c6250dcaf06c238b3d8365d17d646f95cb1874e852b.css integrity="sha256-NvdqrzmhTs9cOjxiUNyvBsI4s9g2XRfWRvlcsYdOhSs=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://AIThoughtLab.github.io/ThinkingAI/css/coder-dark.min.593028e7f7ac55c003b79c230d1cd411bb4ca53b31556c3abb7f027170e646e9.css integrity="sha256-WTAo5/esVcADt5wjDRzUEbtMpTsxVWw6u38CcXDmRuk=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=https://AIThoughtLab.github.io/ThinkingAI/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://AIThoughtLab.github.io/ThinkingAI/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=https://AIThoughtLab.github.io/ThinkingAI/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=https://AIThoughtLab.github.io/ThinkingAI/images/apple-touch-icon.png><link rel=manifest href=https://AIThoughtLab.github.io/ThinkingAI/site.webmanifest><link rel=mask-icon href=https://AIThoughtLab.github.io/ThinkingAI/images/safari-pinned-tab.svg color=#5bbad5><meta name=generator content="Hugo 0.111.3"></head><body class="preload-transitions colorscheme-dark"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://AIThoughtLab.github.io/ThinkingAI/>AI for HUMANITY</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/about/>About</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/physics/>PINN</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/innovations/>Innovations</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/personal/>Personal</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://AIThoughtLab.github.io/ThinkingAI/posts/pose/>Human 3D Pose Reconstruction</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2020-12-26T00:00:00Z>December 26, 2020</time></span>
<span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
6-minute read</span></div><div class=categories><i class="fa fa-folder" aria-hidden=true></i>
<a href=https://AIThoughtLab.github.io/ThinkingAI/categories/artificial-intelligence/>Artificial Intelligence</a></div><div class=tags><i class="fa fa-tag" aria-hidden=true></i>
<span class=tag><a href=https://AIThoughtLab.github.io/ThinkingAI/tags/computer-vision/>Computer Vision</a></span>
<span class=separator>•</span>
<span class=tag><a href=https://AIThoughtLab.github.io/ThinkingAI/tags/3d-reconstruction/>3D Reconstruction</a></span>
<span class=separator>•</span>
<span class=tag><a href=https://AIThoughtLab.github.io/ThinkingAI/tags/pose-estimation/>Pose Estimation</a></span></div></div></header><div class=post-content><p><strong>Human 3D Pose Reconstruction</strong></p><p>This blog focuses on performing 3D reconstruction of human poses using multiple views captured by a calibrated system. To achieve this, a deep neural network called OpenPose is used to estimate the location of the joints in the human body. The OpenPose model has been trained to recognize 25 different joints in the human body, such as the head, shoulders, elbows, hips, knees, and ankles. These joints are critical for determining the posture and movement of a person.</p><p>In 3D reconstruction, a calibrated system means that the parameters such as focal length, sensor information, and lens distortion are known in advance. This enables us to accurately measure the position and orientation of each camera, allowing for precise multi-view reconstruction of the human pose. Accurately estimating the joint locations of a person in 3D space has numerous applications in fields such as motion capture, virtual reality, and human-computer interaction.</p><figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/joins.png></figure><p>In this blog, Structure from Motion (SfM) will be used to complete the task of 3D reconstruction. SfM is a technique used in computer vision to estimate the 3D structure of a scene from a sequence of images captured by a moving camera. In SfM, the camera poses are estimated first, followed by the reconstruction of the 3D scene. This is done by analyzing the displacement vector, or optical flow field, between the images captured from different viewpoints. The goal of SfM is to estimate the 3D structure of a scene by analyzing the changes in camera poses and corresponding image features. For example, like in the figure below, there are several images that are taken from different perspective and reconstructed to create a 3D object.</p><figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/sfm.png></figure><p>The central idea of this approach is to identify the critical points within a sequence of image data or a movie. By tracing these key points, we can then determine both the location of the cameras and the 3D coordinates of the identified points. With this information in hand, we are able to reconstruct the 3D geometry of the scene. Essentially, this technique enables us to extract valuable data from 2D images and convert it into a 3D representation of the scene.</p><p><strong>2D key-points matching</strong>
Here the objective is to read the 2D joints detected by openPose and visualize it on the corresponding video to ensure the 2D data is correct. As we said there are 25 joints and the 2D coordinates of the output is in the form of; xJ1 yJ1 rJ1 xJ2 yJ2 rJ2 xJ3 yJ3 rJ3&mldr;&mldr;. xJ25 yJ25 rJ25.
Here xJ1 yJ1 are the x & y coordinates of the joints and the r is the reliability score. For example, following is the dataset of <strong>squat_1_0.0.txt</strong> <a href="https://drive.google.com/drive/folders/1ig_AGpMrr6RsRXpzJ9etxeNkGbWNJbnHcI?usp=share_link">(link)</a>. Here 333.318, 98.4255 are the x & y coordinate, and the 0.906826 is the reliability score.</p><figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/short.png></figure><p>The following function was used to extract the 2D coordinate positions. It takes three argument; the location of the text file where the coordinate information are saved, video path and the name of the output file to be generated.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=font-weight:700>def</span> keyPoints2D(path, videoPath, output):
</span></span><span style=display:flex><span>  <span style=font-weight:700>with</span> open(path) <span style=font-weight:700>as</span> f:
</span></span><span style=display:flex><span>    lines = []
</span></span><span style=display:flex><span>    <span style=font-weight:700>for</span> line <span style=font-weight:700>in</span> f:
</span></span><span style=display:flex><span>      lines.append(line)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    cap = cv2.VideoCapture(videoPath)
</span></span><span style=display:flex><span>    <span style=font-style:italic># choose codec according to format needed</span>
</span></span><span style=display:flex><span>    fourcc = cv2.VideoWriter_fourcc(<span style=font-style:italic>&#39;M&#39;</span>,<span style=font-style:italic>&#39;J&#39;</span>,<span style=font-style:italic>&#39;P&#39;</span>,<span style=font-style:italic>&#39;G&#39;</span>)
</span></span><span style=display:flex><span>    video = cv2.VideoWriter(<span style=font-style:italic>f</span><span style=font-style:italic>&#39;</span><span style=font-weight:700;font-style:italic>{</span>output<span style=font-weight:700;font-style:italic>}</span><span style=font-style:italic>.avi&#39;</span>,fourcc, int(cap.get(5)), (int(cap.get(3)),int(cap.get(4))))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    count = 0
</span></span><span style=display:flex><span>    counter = 0
</span></span><span style=display:flex><span>    bad_frame = 0
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=font-weight:700>while</span> cap.isOpened():
</span></span><span style=display:flex><span>        ret, frame = cap.read()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-weight:700>if</span> ret == <span style=font-weight:700>True</span>:
</span></span><span style=display:flex><span>          coordinates = []
</span></span><span style=display:flex><span>          
</span></span><span style=display:flex><span>          <span style=font-weight:700>for</span> i <span style=font-weight:700>in</span> lines[counter].split():
</span></span><span style=display:flex><span>            <span style=font-weight:700>if</span> i &lt;= str(1.0):
</span></span><span style=display:flex><span>              <span style=font-weight:700>pass</span>
</span></span><span style=display:flex><span>            <span style=font-weight:700>else</span>:
</span></span><span style=display:flex><span>              coordinates.append(int(float(i)))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>          pairs = list(zip(coordinates[::2], coordinates[1::2]))
</span></span><span style=display:flex><span>          pts = np.array(pairs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>          <span style=font-weight:700>for</span> p <span style=font-weight:700>in</span> pts:
</span></span><span style=display:flex><span>            frame_ = cv2.circle(frame,tuple(p),2,(0,255,255), thickness=-1)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=font-weight:700>if</span> set(p) == set(list(pts[-1])):
</span></span><span style=display:flex><span>              video.write(frame_)
</span></span><span style=display:flex><span>              <span style=font-style:italic>#cv2_imshow(frame_)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>          counter +=1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-weight:700>else</span>:
</span></span><span style=display:flex><span>            cap.release()
</span></span><span style=display:flex><span>            bad_frame +=1
</span></span><span style=display:flex><span>            <span style=font-weight:700>break</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>#print(&#34;Number of bad frames: &#34;, bad_frame)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=font-style:italic>#cv2_imshow(gray)</span>
</span></span><span style=display:flex><span>        <span style=font-weight:700>if</span> cv2.waitKey(1) &amp; 0xFF == ord(<span style=font-style:italic>&#39;q&#39;</span>):
</span></span><span style=display:flex><span>          <span style=font-weight:700>break</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    cap.release()
</span></span><span style=display:flex><span>    cv2.destroyAllWindows()
</span></span></code></pre></div><p>We also define another function to get the data. In our case this function will choose only the <strong>&ldquo;squat&rdquo;</strong> text data or related video files and finally, we will execute it as follow;</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=font-weight:700>def</span> getData(path):
</span></span><span style=display:flex><span>  data_list = []
</span></span><span style=display:flex><span>  <span style=font-weight:700>for</span> data <span style=font-weight:700>in</span> sorted(os.listdir(path)):
</span></span><span style=display:flex><span>    <span style=font-weight:700>if</span> data.startswith(<span style=font-style:italic>&#39;squat&#39;</span>):
</span></span><span style=display:flex><span>      data_list.append(data)
</span></span><span style=display:flex><span>  <span style=font-weight:700>return</span> data_list
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>vidList = getData(videoPath)
</span></span><span style=display:flex><span>textList = getData(textPath)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>for</span> i <span style=font-weight:700>in</span> range(len(vidList)):
</span></span><span style=display:flex><span>  fileName = <span style=font-style:italic>&#39;&#39;</span>.join(vidList[i].split(<span style=font-style:italic>&#34;.avi&#34;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  textPath_ = textPath+textList[i]
</span></span><span style=display:flex><span>  videoPath_ = videoPath+vidList[i]
</span></span><span style=display:flex><span>  keyPoints2D(textPath_, videoPath_, fileName)
</span></span></code></pre></div><p>In the following video, we can see that the 2D points are embedded.</p><video class=video-shortcode preload=auto controls>
<source src=https://AIThoughtLab.github.io/ThinkingAI/images/video.mp4 type=video/mp4>There should have been a video here but your browser does not seem
to support it.</video><p>Next we use the configuration file of cameras to shift the origin of 3D joints (pre-estimated) having reference on camera 0 to reference of other cameras (1 2 3) and project the 3D joints back to image. The purpose is to make sure we understand the translation, rotation between camera and the projection matrix. So let us use the configuration file of cameras to shift the origin of 3D joints from the reference zero camera to the another reference cameras, may be 1 or 2 and project the 3D joints back into the image. The below is the output of the origin shift matrix from 0 to 1. You can see this is a 3x4 matrix.</p><figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/metrix.png></figure><p>We can use this matrix along with the intrinsic parameter of the cameras, the 2D pose on view 2 for example, 3D pose after triangulation (reference at view 0), and retrieve the 3D points at view 0. The we can recall the origin shifting method which was define like below to shift the origin from view 0 to view 1or 1 to 2 etc.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=font-weight:700>def</span> originShift(point,P):
</span></span><span style=display:flex><span>    point = np.append(point, np.array([1]))
</span></span><span style=display:flex><span>    point = np.dot(P,point)
</span></span><span style=display:flex><span>    <span style=font-weight:700>return</span> point
</span></span></code></pre></div><p>Finally after the normalization, it can be projected into 2D point on view2 (or any other), using the intrinsic of camera 2 (or any other). The below is a generated video by the script. The video shows the OpenPose (by big color circle) & the projected point after origin shifting with smaller red circle.</p><video class=video-shortcode preload=auto controls>
<source src=https://AIThoughtLab.github.io/ThinkingAI/images/video1.mp4 type=video/mp4>There should have been a video here but your browser does not seem
to support it.</video><p><strong>3D pose reconstruction</strong></p><p>With the understand of camera projection matrix, let us estimate the 3D joints from the corresponding 2D joints between views and camera projection matrix;
The following steps must be taken in order to reconstruct the 3D coordinates.</p><ol><li>We must estimate the fundamental matrix from key points in two images</li><li>If we have the fundamental matrix and the camera intrinsic matrix, we can calculate the essential matrix.</li><li>If we have the essential matrix, we can calculate the rotation and the translation using single value decomposition (SVD) .</li><li>We can compute the projection matrices using the rotation matrices and the camera calibration matrices.</li><li>With all these information, we can <strong>cv2.triangulatePoints</strong> to get the 3D coordinates (world coordinates).</li></ol><p>The following code snippet helps us to reconstruct the 3D world coordinates.</p><figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/matrix.png></figure><p>The following video is a reconstructed one between the view 0 & view 1. In this reconstruction you can notice that we have used P01, when we reconstruct the P2 projection matrix.</p><video class=video-shortcode preload=auto controls>
<source src=https://AIThoughtLab.github.io/ThinkingAI/images/video2.mp4 type=video/mp4>There should have been a video here but your browser does not seem
to support it.</video><p>References:</p><ol><li><p><a href=http://www.cse.psu.edu/~rtc12/CSE486/lecture25.pdf>http://www.cse.psu.edu/~rtc12/CSE486/lecture25.pdf</a></p></li><li><p><a href=https://stackoverflow.com/questions/18018924/projection-matrix-from-fundamental-matrix>https://stackoverflow.com/questions/18018924/projection-matrix-from-fundamental-matrix</a></p></li><li><p><a href=https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#triangulatepoints>https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#triangulatepoints</a></p></li><li><p><a href="https://www.youtube.com/watch?v=kq3c6QpcAGc&amp;t=543s">https://www.youtube.com/watch?v=kq3c6QpcAGc&amp;t=543s</a></p></li></ol></div><footer></footer></article></section></div><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></main><script src=https://AIThoughtLab.github.io/ThinkingAI/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>