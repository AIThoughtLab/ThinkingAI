<!doctype html><html lang=en><head><title>TensorFlow and the Low Level API - Part 2 · AI for HUMANITY</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Mohamed ABDUL GAFOOR"><meta name=description content="As a continuation from the previous part-1, in this post we will discuss a full multi-class classification problem (all 10 classes for Fashion MNIST). Using TensorFlow’s low level API (in graph mode) let us build a multi-layer neural network. We will define our architecture as follow:
Layer 1: 300 neurons (ReLu activation functions). Layer 2: 100 neurons (ReLu activation function) Layer 3: Softmax Layer Learning rate: 0.01 (with Gradient Descent). We will import the necessary libraries first;"><meta name=keywords content="blog,developer,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="TensorFlow and the Low Level API - Part 2"><meta name=twitter:description content="As a continuation from the previous part-1, in this post we will discuss a full multi-class classification problem (all 10 classes for Fashion MNIST). Using TensorFlow’s low level API (in graph mode) let us build a multi-layer neural network. We will define our architecture as follow:
Layer 1: 300 neurons (ReLu activation functions). Layer 2: 100 neurons (ReLu activation function) Layer 3: Softmax Layer Learning rate: 0.01 (with Gradient Descent). We will import the necessary libraries first;"><meta property="og:title" content="TensorFlow and the Low Level API - Part 2"><meta property="og:description" content="As a continuation from the previous part-1, in this post we will discuss a full multi-class classification problem (all 10 classes for Fashion MNIST). Using TensorFlow’s low level API (in graph mode) let us build a multi-layer neural network. We will define our architecture as follow:
Layer 1: 300 neurons (ReLu activation functions). Layer 2: 100 neurons (ReLu activation function) Layer 3: Softmax Layer Learning rate: 0.01 (with Gradient Descent). We will import the necessary libraries first;"><meta property="og:type" content="article"><meta property="og:url" content="https://AIThoughtLab.github.io/ThinkingAI/posts/fashion-mnist_tf2/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-04-03T00:00:00+00:00"><meta property="article:modified_time" content="2019-04-03T00:00:00+00:00"><link rel=canonical href=https://AIThoughtLab.github.io/ThinkingAI/posts/fashion-mnist_tf2/><link rel=preload href="https://AIThoughtLab.github.io/ThinkingAI/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=https://AIThoughtLab.github.io/ThinkingAI/css/coder.min.36f76aaf39a14ecf5c3a3c6250dcaf06c238b3d8365d17d646f95cb1874e852b.css integrity="sha256-NvdqrzmhTs9cOjxiUNyvBsI4s9g2XRfWRvlcsYdOhSs=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://AIThoughtLab.github.io/ThinkingAI/css/coder-dark.min.593028e7f7ac55c003b79c230d1cd411bb4ca53b31556c3abb7f027170e646e9.css integrity="sha256-WTAo5/esVcADt5wjDRzUEbtMpTsxVWw6u38CcXDmRuk=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=https://AIThoughtLab.github.io/ThinkingAI/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://AIThoughtLab.github.io/ThinkingAI/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=https://AIThoughtLab.github.io/ThinkingAI/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=https://AIThoughtLab.github.io/ThinkingAI/images/apple-touch-icon.png><link rel=manifest href=https://AIThoughtLab.github.io/ThinkingAI/site.webmanifest><link rel=mask-icon href=https://AIThoughtLab.github.io/ThinkingAI/images/safari-pinned-tab.svg color=#5bbad5><meta name=generator content="Hugo 0.111.2"></head><body class="preload-transitions colorscheme-dark"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://AIThoughtLab.github.io/ThinkingAI/>AI for HUMANITY</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/about/>About</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/innovations/>Innovations</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/personal/>Personal</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://AIThoughtLab.github.io/ThinkingAI/posts/fashion-mnist_tf2/>TensorFlow and the Low Level API - Part 2</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2019-04-03T00:00:00Z>April 3, 2019</time></span>
<span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
6-minute read</span></div><div class=categories><i class="fa fa-folder" aria-hidden=true></i>
<a href=https://AIThoughtLab.github.io/ThinkingAI/categories/artificial-intelligence/>Artificial Intelligence</a></div><div class=tags><i class="fa fa-tag" aria-hidden=true></i>
<span class=tag><a href=https://AIThoughtLab.github.io/ThinkingAI/tags/tensorflow/>TensorFlow</a></span>
<span class=separator>•</span>
<span class=tag><a href=https://AIThoughtLab.github.io/ThinkingAI/tags/deep-learning/>Deep Learning</a></span>
<span class=separator>•</span>
<span class=tag><a href=https://AIThoughtLab.github.io/ThinkingAI/tags/mnist/>MNIST</a></span></div></div></header><div class=post-content><p>As a continuation from the previous part-1, in this post we will discuss a full <strong>multi-class classification</strong> problem (all 10 classes for Fashion MNIST). Using TensorFlow’s low level API (in graph mode) let us build a multi-layer neural network. We will define our architecture as follow:</p><ul><li>Layer 1: 300 neurons (ReLu activation functions).</li><li>Layer 2: 100 neurons (ReLu activation function)</li><li>Layer 3: Softmax Layer</li><li>Learning rate: 0.01 (with Gradient Descent).</li></ul><p>We will import the necessary libraries first;</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=font-weight:700>import</span> <span style=font-weight:700>tensorflow</span> <span style=font-weight:700>as</span> <span style=font-weight:700>tf</span>
</span></span><span style=display:flex><span><span style=font-weight:700>import</span> <span style=font-weight:700>numpy</span> <span style=font-weight:700>as</span> <span style=font-weight:700>np</span>
</span></span><span style=display:flex><span><span style=font-weight:700>import</span> <span style=font-weight:700>pylab</span> 
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>sklearn</span> <span style=font-weight:700>import</span> preprocessing
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>sklearn.model_selection</span> <span style=font-weight:700>import</span> train_test_split
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>sklearn.preprocessing</span> <span style=font-weight:700>import</span> StandardScaler
</span></span><span style=display:flex><span><span style=font-weight:700>import</span> <span style=font-weight:700>matplotlib.pyplot</span> <span style=font-weight:700>as</span> <span style=font-weight:700>plt</span>
</span></span><span style=display:flex><span>%matplotlib inline
</span></span><span style=display:flex><span><span style=font-weight:700>import</span> <span style=font-weight:700>time</span>
</span></span><span style=display:flex><span>tf.reset_default_graph()
</span></span></code></pre></div><p>Then we will write a small function to load the data, where we reshape, normalize and convert the labels to one-hot-encoding.
<strong>One-hot encoding</strong> is a technique used in machine learning to represent categorical variables as numerical data. In one-hot encoding, a categorical variable with N possible values is transformed into N binary variables, with each binary variable representing one of the N possible values. Only one of the N binary variables is &ldquo;hot&rdquo; or &ldquo;on&rdquo;, which means it is set to 1, and the rest of the variables are set to 0. This results in a binary vector of length N that uniquely represents the categorical variable.</p><p>For example, let us say there are A, B, and C, and one-hot encoding would transform this variable into three binary variables: [1, 0, 0], [0, 1, 0], [0, 0, 1]. The problem with this encoding technique is that, it can result in a large number of features and increase the dimensionality of the data, which can slow down the training process. To mitigate this issue, techniques such as dimensionality reduction and feature selection can be applied.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=font-weight:700>def</span> loadData():
</span></span><span style=display:flex><span>    fashion_mnist = tf.keras.datasets.fashion_mnist
</span></span><span style=display:flex><span>    (train_images, train_labels), (x_test, y_test) = fashion_mnist.load_data()
</span></span><span style=display:flex><span>    print(train_images.shape)
</span></span><span style=display:flex><span>    x_train, x_valid, y_train, y_valid = train_test_split(train_images, train_labels, test_size=0.1, random_state=1)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=font-style:italic>#reshape the data</span>
</span></span><span style=display:flex><span>    x_train = x_train.reshape(x_train.shape[0], -1).astype(<span style=font-style:italic>&#39;float32&#39;</span>)
</span></span><span style=display:flex><span>    x_valid = x_valid.reshape(x_valid.shape[0], -1).astype(<span style=font-style:italic>&#39;float32&#39;</span>)
</span></span><span style=display:flex><span>    x_test = x_test.reshape(x_test.shape[0], -1).astype(<span style=font-style:italic>&#39;float32&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=font-style:italic># Normalize</span>
</span></span><span style=display:flex><span>    x_train = x_train/255.0
</span></span><span style=display:flex><span>    x_valid = x_valid/255.0
</span></span><span style=display:flex><span>    x_test = x_test/255.0
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=font-style:italic>#Transpose of the matrix</span>
</span></span><span style=display:flex><span>    x_train = x_train.T
</span></span><span style=display:flex><span>    x_valid = x_valid.T
</span></span><span style=display:flex><span>    x_test = x_test.T
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=font-style:italic># Convert labels to one-hot-encoded</span>
</span></span><span style=display:flex><span>    number_of_classes = 10
</span></span><span style=display:flex><span>    y_train = tf.keras.utils.to_categorical(y_train, number_of_classes)
</span></span><span style=display:flex><span>    y_valid = tf.keras.utils.to_categorical(y_valid, number_of_classes)
</span></span><span style=display:flex><span>    y_test = tf.keras.utils.to_categorical(y_test, number_of_classes)
</span></span><span style=display:flex><span>    print(y_train[0])
</span></span><span style=display:flex><span>    print(y_train[1])
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=font-style:italic># transpose the labels</span>
</span></span><span style=display:flex><span>    y_train = y_train.T
</span></span><span style=display:flex><span>    y_valid = y_valid.T
</span></span><span style=display:flex><span>    y_test = y_test.T
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    print (<span style=font-style:italic>&#34;Reshaped Data: &#34;</span>)
</span></span><span style=display:flex><span>    print(x_train.shape, y_train.shape, x_valid.shape, y_valid.shape, x_test.shape, y_test.shape)
</span></span><span style=display:flex><span>    <span style=font-style:italic>#print (train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)</span>
</span></span><span style=display:flex><span>    <span style=font-weight:700>return</span> x_train, y_train, x_valid, y_valid, x_test, y_test
</span></span></code></pre></div><p>Now we will build our model, where input vector is 784, in the first hidden layer there are 300 neurons and 100 in the second hidden layer. Finally the output layer has 10 neurons as there are 10 classes;</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=font-style:italic># Model building</span>
</span></span><span style=display:flex><span><span style=font-weight:700>def</span> main(x_train, y_train, x_valid, y_valid, x_test, y_test):
</span></span><span style=display:flex><span>    n_inputs = 784
</span></span><span style=display:flex><span>    n_hidden_1 = 300
</span></span><span style=display:flex><span>    n_hidden_2 = 100
</span></span><span style=display:flex><span>    n_output = 10
</span></span><span style=display:flex><span>    learningRate = 0.01
</span></span><span style=display:flex><span>    n_epochs = 40
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>    <span style=font-style:italic># Placeholders for the traing and label data    </span>
</span></span><span style=display:flex><span>    x = tf.placeholder(tf.float32, [n_inputs, <span style=font-weight:700>None</span>], name=<span style=font-style:italic>&#39;image&#39;</span>)    
</span></span><span style=display:flex><span>    y = tf.placeholder(tf.float32, [n_output, <span style=font-weight:700>None</span>], name=<span style=font-style:italic>&#39;label&#39;</span>)
</span></span><span style=display:flex><span>   <span style=font-style:italic># tf.get_variable(&#34;W1&#34;, [n_hidden, n_inputs],initializer = tf.glorot_uniform_initializer(seed=1) )</span>
</span></span><span style=display:flex><span>    <span style=font-style:italic># Create weight and bias matrices (variables) for each layer of our network</span>
</span></span><span style=display:flex><span>    W1 = tf.Variable(tf.random_normal([n_hidden_1, n_inputs], mean=0.0, stddev= 0.9)) <span style=font-style:italic># 784 = 28 * 28</span>
</span></span><span style=display:flex><span>    b1 = tf.Variable(tf.zeros([n_hidden_1, 1]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    W2 = tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1], mean=0.0, stddev= 0.05)) <span style=font-style:italic># 784 = 28 * 28</span>
</span></span><span style=display:flex><span>    b2 = tf.Variable(tf.zeros([n_hidden_2, 1]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    W3 = tf.Variable(tf.random_normal([n_output, n_hidden_2], mean=0.0, stddev=0.005)) <span style=font-style:italic># 784 = 28 * 28</span>
</span></span><span style=display:flex><span>    b3 = tf.Variable(tf.zeros([n_output, 1]))
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>    <span style=font-style:italic># Push feature data through layers of NN</span>
</span></span><span style=display:flex><span>    layer_1  = tf.nn.relu(tf.add(tf.matmul(W1, x), b1))
</span></span><span style=display:flex><span>    layer_2 = tf.nn.relu(tf.add(tf.matmul(W2, layer_1), b2))
</span></span><span style=display:flex><span>    layer_3 = tf.add(tf.matmul(W3, layer_2), b3)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    soft_max = tf.nn.softmax(layer_3)
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>    err = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=layer_3)
</span></span><span style=display:flex><span>    loss = tf.reduce_mean(err)
</span></span><span style=display:flex><span>    optimizer = tf.train.GradientDescentOptimizer(learningRate).minimize(loss)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=font-style:italic># Calculate the correct predictions</span>
</span></span><span style=display:flex><span>    correct_prediction = tf.equal(tf.argmax(soft_max), tf.argmax(y))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>   <span style=font-style:italic># Calculate accuracy on the test set</span>
</span></span><span style=display:flex><span>    accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span style=font-style:italic>&#34;float&#34;</span>))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=font-weight:700>with</span> tf.Session() <span style=font-weight:700>as</span> sess:
</span></span><span style=display:flex><span>        train_loss_list = []
</span></span><span style=display:flex><span>        val_loss_list = []
</span></span><span style=display:flex><span>        epoch_list = []
</span></span><span style=display:flex><span>        <span style=font-style:italic>#epoch_list1 = []</span>
</span></span><span style=display:flex><span>        train_acc_list = []
</span></span><span style=display:flex><span>        val_acc_list = []
</span></span><span style=display:flex><span>        start_time = time.time()
</span></span><span style=display:flex><span>        sess.run(tf.global_variables_initializer())
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=font-weight:700>for</span> epoch <span style=font-weight:700>in</span> range(n_epochs):
</span></span><span style=display:flex><span>            _, train_loss, train_acc  = sess.run([optimizer, loss, accuracy], feed_dict={x: x_train, y: y_train})
</span></span><span style=display:flex><span>            _, val_loss, val_acc  = sess.run([optimizer, loss, accuracy], feed_dict={x: x_valid, y: y_valid})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            train_loss_list.append(train_loss)
</span></span><span style=display:flex><span>            epoch_list.append(epoch)
</span></span><span style=display:flex><span>            train_acc_list.append(train_acc)
</span></span><span style=display:flex><span>            <span style=font-style:italic>##-------------------------------</span>
</span></span><span style=display:flex><span>            val_loss_list.append(val_loss)
</span></span><span style=display:flex><span>            <span style=font-style:italic>#epoch_list.append(epoch)</span>
</span></span><span style=display:flex><span>            val_acc_list.append(val_acc) 
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            print (<span style=font-style:italic>&#34;Epoch &#34;</span>, epoch, <span style=font-style:italic>&#34; Train Loss: &#34;</span>, train_loss, <span style=font-style:italic>&#34;  Train Acc: &#34;</span>, train_acc)
</span></span><span style=display:flex><span>            print (<span style=font-style:italic>&#34;Epoch &#34;</span>, epoch, <span style=font-style:italic>&#34; Val Loss: &#34;</span>, val_loss, <span style=font-style:italic>&#34;  Val Acc: &#34;</span>, val_acc)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        duration = time.time() - start_time
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        pylab.plot(epoch_list, val_loss_list, <span style=font-style:italic>&#39;-or&#39;</span>, label = <span style=font-style:italic>&#39;val_loss&#39;</span>)
</span></span><span style=display:flex><span>        pylab.plot(epoch_list, train_acc_list, <span style=font-style:italic>&#39;-ob&#39;</span>, label = <span style=font-style:italic>&#39;train_accuracy&#39;</span>)
</span></span><span style=display:flex><span>        pylab.plot(epoch_list, val_acc_list, <span style=font-style:italic>&#39;-ok&#39;</span>, label = <span style=font-style:italic>&#39;val_accuracy&#39;</span>)
</span></span><span style=display:flex><span>        pylab.plot(epoch_list, train_loss_list, <span style=font-style:italic>&#39;purple&#39;</span>, label = <span style=font-style:italic>&#39;train_loss_list&#39;</span>)
</span></span><span style=display:flex><span>        pylab.legend(loc=<span style=font-style:italic>&#39;bottom right&#39;</span>)
</span></span><span style=display:flex><span>        plt.xlabel(<span style=font-style:italic>&#39;Epochs&#39;</span>)
</span></span><span style=display:flex><span>        pylab.show()
</span></span><span style=display:flex><span>        print(<span style=font-style:italic>&#34;Total time is: &#34;</span>, round(duration, 2), <span style=font-style:italic>&#34;seconds&#34;</span>)
</span></span><span style=display:flex><span>        print (<span style=font-style:italic>&#34;Final Validation Accuracy &#34;</span>, sess.run(accuracy, feed_dict={x: x_valid, y: y_valid}))
</span></span><span style=display:flex><span>        print (<span style=font-style:italic>&#34;Final Test Accuracy &#34;</span>, sess.run(accuracy, feed_dict={x: x_test, y: y_test}))
</span></span></code></pre></div><p>Following is the output.<figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/keras.png></figure></p><p>Hence the final validation accuracy is 0.6755 and the final test accuracy is 0.6767.</p><p><strong>Mini-batch</strong></p><p>Mini-batch training is a good approach in deep learning because it balances the trade-off between computational efficiency and stability of the gradients. It involves dividing the training dataset into small batches, which are processed in parallel to compute the gradients. This allows for faster training times as compared to batch training (where the entire dataset is processed at once), while still preserving the stability of gradients and avoiding the fluctuations seen in stochastic gradient descent (where only a single sample is processed at once). Mini-batch training also enables the use of parallel computing resources, such as GPUs, to speed up training. If we introduce a mini batch size of 100, it is indeed possible to increase the accuracy of the model. When we test, it was around 0.7287. Introducing batch size is easy. Following is the code snippet.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=font-weight:700>def</span> getBatch(x, y, begin, end):
</span></span><span style=display:flex><span>    x_miniBatch = x[begin:end]
</span></span><span style=display:flex><span>    y_miniBatch = y[begin:end]
</span></span><span style=display:flex><span>    <span style=font-weight:700>return</span> x_miniBatch, y_miniBatch
</span></span></code></pre></div><p>There is no much change to the previous code except some modification should be done to the <strong>tf.Session()</strong>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>    <span style=font-weight:700>with</span> tf.Session() <span style=font-weight:700>as</span> sess:
</span></span><span style=display:flex><span>        loss_list = []
</span></span><span style=display:flex><span>        epoch_list = []
</span></span><span style=display:flex><span>        acc_list = []
</span></span><span style=display:flex><span>        start_time = time.time()
</span></span><span style=display:flex><span>        sess.run(tf.global_variables_initializer())
</span></span><span style=display:flex><span>        iteration = int(len(x_train)/batchSize)
</span></span><span style=display:flex><span>        <span style=font-weight:700>for</span> epoch <span style=font-weight:700>in</span> range(n_epochs):
</span></span><span style=display:flex><span>            <span style=font-style:italic>#print(&#39;Training epoch: {}&#39;.format(epoch + 1))</span>
</span></span><span style=display:flex><span>            <span style=font-weight:700>for</span> i <span style=font-weight:700>in</span> range(iteration):
</span></span><span style=display:flex><span>                begin = i * batchSize
</span></span><span style=display:flex><span>                end = (i + 1) * batchSize
</span></span><span style=display:flex><span>                x_short, y_short = getBatch(x_train, y_train, begin, end)
</span></span><span style=display:flex><span>                x_short = x_short.T
</span></span><span style=display:flex><span>                y_short = y_short.T
</span></span><span style=display:flex><span>                feed_dict = {x: x_short, y:y_short}
</span></span><span style=display:flex><span>                sess.run(optimizer, feed_dict=feed_dict)
</span></span><span style=display:flex><span>                
</span></span><span style=display:flex><span>                <span style=font-weight:700>if</span> i % 100 == 0:
</span></span><span style=display:flex><span>                    loss_after_batch, acc_after_batch = sess.run([loss, accuracy],feed_dict=feed_dict)
</span></span><span style=display:flex><span>                    <span style=font-style:italic>#print(&#39;iter: &#39;, i, &#39;Loss: &#39;, loss_after_batch, &#39;Training Accuracy: &#39;, acc_after_batch)</span>
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            feed_dict_test = {x: x_test.T, y: y_test.T}
</span></span><span style=display:flex><span>            loss_test, acc_test = sess.run([loss, accuracy], feed_dict=feed_dict_test)
</span></span><span style=display:flex><span>            <span style=font-style:italic>#print(&#39;---------*************---------&#39;)</span>
</span></span><span style=display:flex><span>            <span style=font-style:italic>#print(&#34;Test Loss: &#34;, loss_test, &#34;Test Accuracy: &#34;, acc_test)</span>
</span></span><span style=display:flex><span>            <span style=font-weight:700>return</span> acc_test
</span></span></code></pre></div><p>Thats it!! Now we know how to use mini-batch in our ML/DL problems.</p></div><footer></footer></article></section></div><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></main><script src=https://AIThoughtLab.github.io/ThinkingAI/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>