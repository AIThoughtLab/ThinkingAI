<!doctype html><html lang=en><head><title>Keras – High Level API · AI for HUMANITY</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Mohamed ABDUL GAFOOR"><meta name=description content="What is Keras?
It is a high-level neural network API, written in Python and able to running on top of TensorFlow. It is a very useful API, which enables us fast experimentation. Keras acts as an interface for the TensorFlow library. The task here is to study the image classification problem using notMNIST dataset. This dataset contains images of letters from A – J inclusive (dowload here). See the figure below to get an idea;"><meta name=keywords content="blog,developer,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="Keras – High Level API"><meta name=twitter:description content="What is Keras?
It is a high-level neural network API, written in Python and able to running on top of TensorFlow. It is a very useful API, which enables us fast experimentation. Keras acts as an interface for the TensorFlow library. The task here is to study the image classification problem using notMNIST dataset. This dataset contains images of letters from A – J inclusive (dowload here). See the figure below to get an idea;"><meta property="og:title" content="Keras – High Level API"><meta property="og:description" content="What is Keras?
It is a high-level neural network API, written in Python and able to running on top of TensorFlow. It is a very useful API, which enables us fast experimentation. Keras acts as an interface for the TensorFlow library. The task here is to study the image classification problem using notMNIST dataset. This dataset contains images of letters from A – J inclusive (dowload here). See the figure below to get an idea;"><meta property="og:type" content="article"><meta property="og:url" content="https://AIThoughtLab.github.io/ThinkingAI/posts/kerasapi/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-04-10T00:00:00+00:00"><meta property="article:modified_time" content="2019-04-10T00:00:00+00:00"><link rel=canonical href=https://AIThoughtLab.github.io/ThinkingAI/posts/kerasapi/><link rel=preload href="https://AIThoughtLab.github.io/ThinkingAI/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=https://AIThoughtLab.github.io/ThinkingAI/css/coder.min.36f76aaf39a14ecf5c3a3c6250dcaf06c238b3d8365d17d646f95cb1874e852b.css integrity="sha256-NvdqrzmhTs9cOjxiUNyvBsI4s9g2XRfWRvlcsYdOhSs=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://AIThoughtLab.github.io/ThinkingAI/css/coder-dark.min.593028e7f7ac55c003b79c230d1cd411bb4ca53b31556c3abb7f027170e646e9.css integrity="sha256-WTAo5/esVcADt5wjDRzUEbtMpTsxVWw6u38CcXDmRuk=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=https://AIThoughtLab.github.io/ThinkingAI/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://AIThoughtLab.github.io/ThinkingAI/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=https://AIThoughtLab.github.io/ThinkingAI/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=https://AIThoughtLab.github.io/ThinkingAI/images/apple-touch-icon.png><link rel=manifest href=https://AIThoughtLab.github.io/ThinkingAI/site.webmanifest><link rel=mask-icon href=https://AIThoughtLab.github.io/ThinkingAI/images/safari-pinned-tab.svg color=#5bbad5><meta name=generator content="Hugo 0.110.0"></head><body class="preload-transitions colorscheme-dark"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://AIThoughtLab.github.io/ThinkingAI/>AI for HUMANITY</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/about/>About</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/innovations/>Innovations</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/personal/>Personal</a></li><li class=navigation-item><a class=navigation-link href=https://AIThoughtLab.github.io/ThinkingAI/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://AIThoughtLab.github.io/ThinkingAI/posts/kerasapi/>Keras – High Level API</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2019-04-10T00:00:00Z>April 10, 2019</time></span>
<span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
5-minute read</span></div><div class=categories><i class="fa fa-folder" aria-hidden=true></i>
<a href=https://AIThoughtLab.github.io/ThinkingAI/categories/artificial-intelligence/>Artificial Intelligence</a></div><div class=tags><i class="fa fa-tag" aria-hidden=true></i>
<span class=tag><a href=https://AIThoughtLab.github.io/ThinkingAI/tags/keras/>Keras</a></span>
<span class=separator>•</span>
<span class=tag><a href=https://AIThoughtLab.github.io/ThinkingAI/tags/machine-learning/>Machine Learning</a></span>
<span class=separator>•</span>
<span class=tag><a href=https://AIThoughtLab.github.io/ThinkingAI/tags/deep-learning/>Deep Learning</a></span>
<span class=separator>•</span>
<span class=tag><a href=https://AIThoughtLab.github.io/ThinkingAI/tags/notmnist/>notMNIST</a></span></div></div></header><div class=post-content><p><strong>What is Keras?</strong></p><p>It is a high-level neural network API, written in Python and able to running on top of TensorFlow. It is a very useful API, which enables us fast experimentation. Keras acts as an interface for the TensorFlow library. The task here is to study the image classification problem using notMNIST dataset. This dataset contains images of letters from A – J inclusive (<a href="https://drive.google.com/file/d/1QpI1AWvSAn_B61FPpJFPFgmHQ-5s05df/view?usp=share_link">dowload here</a>). See the figure below to get an idea;</p><figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/notMNIST.png></figure><p>There are few pre-processing work has been completed already for this data and on top of it, it has been normalized. In total there are 200,000 training
images and 17,000 test images in this dataset. Image size is 28*28 pixels, so there are 784 features in total.</p><p>Now we will use Keras to build a SoftMax classifier. This will serve as a benchmark for the following parts.</p><p>Import the necessary libraries first.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>keras</span> <span style=font-weight:700>import</span> models
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>keras</span> <span style=font-weight:700>import</span> layers
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>keras.optimizers</span> <span style=font-weight:700>import</span> sgd
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>sklearn</span> <span style=font-weight:700>import</span> preprocessing
</span></span><span style=display:flex><span><span style=font-weight:700>import</span> <span style=font-weight:700>matplotlib.pyplot</span> <span style=font-weight:700>as</span> <span style=font-weight:700>plt</span>
</span></span><span style=display:flex><span><span style=font-weight:700>import</span> <span style=font-weight:700>pylab</span> 
</span></span><span style=display:flex><span>%matplotlib inline
</span></span><span style=display:flex><span><span style=font-weight:700>import</span> <span style=font-weight:700>time</span>
</span></span><span style=display:flex><span><span style=font-weight:700>import</span> <span style=font-weight:700>tensorflow</span> <span style=font-weight:700>as</span> <span style=font-weight:700>tf</span>
</span></span><span style=display:flex><span>tf.reset_default_graph()
</span></span><span style=display:flex><span><span style=font-weight:700>import</span> <span style=font-weight:700>h5py</span>
</span></span><span style=display:flex><span><span style=font-weight:700>import</span> <span style=font-weight:700>numpy</span> <span style=font-weight:700>as</span> <span style=font-weight:700>np</span>
</span></span></code></pre></div><p>We will write a small function to load the data;</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=font-weight:700>def</span> loadData():
</span></span><span style=display:flex><span>    <span style=font-weight:700>with</span> h5py.File(<span style=font-style:italic>&#39;data.h5&#39;</span>,<span style=font-style:italic>&#39;r&#39;</span>) <span style=font-weight:700>as</span> hf:
</span></span><span style=display:flex><span>        print(<span style=font-style:italic>&#39;List of arrays in this file: </span><span style=font-weight:700;font-style:italic>\n</span><span style=font-style:italic>&#39;</span>, hf.keys())
</span></span><span style=display:flex><span>        allTrain = hf.get(<span style=font-style:italic>&#39;trainData&#39;</span>)
</span></span><span style=display:flex><span>        allTest = hf.get(<span style=font-style:italic>&#39;testData&#39;</span>)
</span></span><span style=display:flex><span>        npTrain = np.array(allTrain)
</span></span><span style=display:flex><span>        npTest = np.array(allTest)
</span></span><span style=display:flex><span>        print(<span style=font-style:italic>&#39;Shape of the array dataset_1: </span><span style=font-weight:700;font-style:italic>\n</span><span style=font-style:italic>&#39;</span>, npTrain.shape)
</span></span><span style=display:flex><span>        print(<span style=font-style:italic>&#39;Shape of the array dataset_2: </span><span style=font-weight:700;font-style:italic>\n</span><span style=font-style:italic>&#39;</span>, npTest.shape)
</span></span><span style=display:flex><span>    <span style=font-weight:700>return</span> npTrain[:,:-1], npTrain[:, -1], npTest[:,:-1], npTest[:, -1]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-style:italic>#Loaded.  </span>
</span></span><span style=display:flex><span>x_train, y_train, x_test, y_test = loadData()   
</span></span></code></pre></div><p>We will initilize the model using keras <strong>models.Sequential()</strong>. There are 10 classes and input shape is at the begining <strong>(784, )</strong> because there are 28*28 pixels. We also set the activation function to <strong>softmax</strong>, optimizer to <strong>adam</strong>. We will take the advantage of batch size, hence set to 256.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>model = models.Sequential()
</span></span><span style=display:flex><span>model.add(layers.Dense(10, activation=<span style=font-style:italic>&#39;softmax&#39;</span>, input_shape=(784,)))
</span></span><span style=display:flex><span>model.compile(optimizer=<span style=font-style:italic>&#39;adam&#39;</span>, loss=<span style=font-style:italic>&#39;sparse_categorical_crossentropy&#39;</span>, metrics=[<span style=font-style:italic>&#39;accuracy&#39;</span>])
</span></span><span style=display:flex><span>history = model.fit(x_train, y_train, epochs=30, batch_size = 256, validation_split=0.1)
</span></span></code></pre></div><p>After we train the model for 30 epochs, we will plot the graph to see the nature of the loss.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>history_dict = history.history
</span></span><span style=display:flex><span>print(<span style=font-style:italic>&#34;Keys: &#34;</span>, history_dict.keys())
</span></span><span style=display:flex><span>loss_values = history_dict[<span style=font-style:italic>&#39;loss&#39;</span>]
</span></span><span style=display:flex><span>val_loss_values = history_dict[<span style=font-style:italic>&#39;val_loss&#39;</span>]
</span></span><span style=display:flex><span>acc_values = history_dict[<span style=font-style:italic>&#39;acc&#39;</span>]
</span></span><span style=display:flex><span>val_acc_values = history_dict[<span style=font-style:italic>&#39;val_acc&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>epochs = range(1, len(history_dict[<span style=font-style:italic>&#39;acc&#39;</span>])+1)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>def</span> plot_results(loss_values,val_loss_values, epochs, **kwargs):
</span></span><span style=display:flex><span>  label1 = kwargs.pop(<span style=font-style:italic>&#39;label1&#39;</span>)
</span></span><span style=display:flex><span>  plt.plot(epochs, loss_values, <span style=font-style:italic>&#39;bo&#39;</span>,label = label1)
</span></span><span style=display:flex><span>  label2 = kwargs.pop(<span style=font-style:italic>&#39;label2&#39;</span>)
</span></span><span style=display:flex><span>  plt.plot(epochs, val_loss_values, <span style=font-style:italic>&#39;r&#39;</span>,label = label2)
</span></span><span style=display:flex><span>  title  = kwargs.pop(<span style=font-style:italic>&#39;title&#39;</span>)
</span></span><span style=display:flex><span>  xlabel = kwargs.pop(<span style=font-style:italic>&#39;xlabel&#39;</span>)
</span></span><span style=display:flex><span>  ylabel = kwargs.pop(<span style=font-style:italic>&#39;ylabel&#39;</span>)
</span></span><span style=display:flex><span>  plt.title(title)
</span></span><span style=display:flex><span>  plt.xlabel(xlabel)
</span></span><span style=display:flex><span>  plt.ylabel(ylabel)
</span></span><span style=display:flex><span>  plt.legend()
</span></span><span style=display:flex><span>  plt.show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-style:italic>#loss  </span>
</span></span><span style=display:flex><span>plot_para = {<span style=font-style:italic>&#39;title&#39;</span>: <span style=font-style:italic>&#39;Training and Validation Loss&#39;</span>, <span style=font-style:italic>&#39;xlabel&#39;</span>:<span style=font-style:italic>&#39;Epochs&#39;</span>, <span style=font-style:italic>&#39;ylabel&#39;</span>: <span style=font-style:italic>&#39;Loss&#39;</span>, <span style=font-style:italic>&#39;label1&#39;</span>: <span style=font-style:italic>&#39;Training Loss&#39;</span>, <span style=font-style:italic>&#39;label2&#39;</span>: <span style=font-style:italic>&#39;Validation Loss&#39;</span>}
</span></span><span style=display:flex><span>plot_results(loss_values,val_loss_values, epochs, **plot_para) 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-style:italic>#accuray</span>
</span></span><span style=display:flex><span>plot_para = {<span style=font-style:italic>&#39;title&#39;</span>: <span style=font-style:italic>&#39;Training and Validation Accuracy&#39;</span>, <span style=font-style:italic>&#39;xlabel&#39;</span>:<span style=font-style:italic>&#39;Epochs&#39;</span>, <span style=font-style:italic>&#39;ylabel&#39;</span>: <span style=font-style:italic>&#39;acc&#39;</span>, <span style=font-style:italic>&#39;label1&#39;</span>: <span style=font-style:italic>&#39;Training acc&#39;</span>, <span style=font-style:italic>&#39;label2&#39;</span>: <span style=font-style:italic>&#39;Validation acc&#39;</span>}
</span></span><span style=display:flex><span>plot_results(acc_values,val_acc_values, epochs, **plot_para) 
</span></span></code></pre></div><p>Following figure below shows the loss and the accuracy for each epochs. We should be able to get an accuracy of 85% in the training data-set and around 83% in the test dataset.<figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/1.png></figure></p><p>Now let us create a 2 layer networks. Layer 1 consists of 200 Neurons (activation=&lsquo;relu&rsquo;), Layer 2 has 10 neurons (activation=&lsquo;softmax&rsquo;). This time we can also test other optimizers, such <strong>sgd</strong>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>model = models.Sequential()
</span></span><span style=display:flex><span>model.add(layers.Dense(200, activation=<span style=font-style:italic>&#39;relu&#39;</span>, input_shape=(784,)))
</span></span><span style=display:flex><span>model.add(layers.Dense(10, activation=<span style=font-style:italic>&#39;softmax&#39;</span>))
</span></span><span style=display:flex><span><span style=font-style:italic># sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)         </span>
</span></span><span style=display:flex><span>model.compile(optimizer=<span style=font-style:italic>&#39;adam&#39;</span>, loss=<span style=font-style:italic>&#39;sparse_categorical_crossentropy&#39;</span>, metrics=[<span style=font-style:italic>&#39;accuracy&#39;</span>])
</span></span><span style=display:flex><span><span style=font-style:italic>#model.compile(loss=&#39;mean_squared_error&#39;, optimizer=sgd)</span>
</span></span><span style=display:flex><span>start_time = time.time()
</span></span><span style=display:flex><span>history = model.fit(x_train, y_train, epochs=30, batch_size = 256, validation_split=0.1)
</span></span><span style=display:flex><span>duration = time.time() - start_time
</span></span><span style=display:flex><span>print(<span style=font-style:italic>&#34;Total time is: &#34;</span>, round(duration, 2), <span style=font-style:italic>&#34;seconds&#34;</span>)
</span></span></code></pre></div><p>This simple modification help us to improve the performance of the network significantly. For the same batch size like before, after 30 epochs, we should be able to get an accuracy of 90%.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>history_dict = history.history
</span></span><span style=display:flex><span>print(<span style=font-style:italic>&#34;Keys: &#34;</span>, history_dict.keys())
</span></span><span style=display:flex><span>loss_values = history_dict[<span style=font-style:italic>&#39;loss&#39;</span>]
</span></span><span style=display:flex><span>val_loss_values = history_dict[<span style=font-style:italic>&#39;val_loss&#39;</span>]
</span></span><span style=display:flex><span>acc_values = history_dict[<span style=font-style:italic>&#39;acc&#39;</span>]
</span></span><span style=display:flex><span>val_acc_values = history_dict[<span style=font-style:italic>&#39;val_acc&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>epochs = range(1, len(history_dict[<span style=font-style:italic>&#39;acc&#39;</span>])+1)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>def</span> plot_results(loss_values,val_loss_values, epochs, **kwargs):
</span></span><span style=display:flex><span>  label1 = kwargs.pop(<span style=font-style:italic>&#39;label1&#39;</span>)
</span></span><span style=display:flex><span>  plt.plot(epochs, loss_values, <span style=font-style:italic>&#39;bo&#39;</span>,label = label1)
</span></span><span style=display:flex><span>  label2 = kwargs.pop(<span style=font-style:italic>&#39;label2&#39;</span>)
</span></span><span style=display:flex><span>  plt.plot(epochs, val_loss_values, <span style=font-style:italic>&#39;r&#39;</span>,label = label2)
</span></span><span style=display:flex><span>  title  = kwargs.pop(<span style=font-style:italic>&#39;title&#39;</span>)
</span></span><span style=display:flex><span>  xlabel = kwargs.pop(<span style=font-style:italic>&#39;xlabel&#39;</span>)
</span></span><span style=display:flex><span>  ylabel = kwargs.pop(<span style=font-style:italic>&#39;ylabel&#39;</span>)
</span></span><span style=display:flex><span>  plt.title(title)
</span></span><span style=display:flex><span>  plt.xlabel(xlabel)
</span></span><span style=display:flex><span>  plt.ylabel(ylabel)
</span></span><span style=display:flex><span>  plt.legend()
</span></span><span style=display:flex><span>  plt.show()
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>plot_para = {<span style=font-style:italic>&#39;title&#39;</span>: <span style=font-style:italic>&#39;Training and Validation Loss&#39;</span>, <span style=font-style:italic>&#39;xlabel&#39;</span>:<span style=font-style:italic>&#39;Epochs&#39;</span>, <span style=font-style:italic>&#39;ylabel&#39;</span>: <span style=font-style:italic>&#39;Loss&#39;</span>, <span style=font-style:italic>&#39;label1&#39;</span>: <span style=font-style:italic>&#39;Training Loss&#39;</span>, <span style=font-style:italic>&#39;label2&#39;</span>: <span style=font-style:italic>&#39;Validation Loss&#39;</span>}
</span></span><span style=display:flex><span>plot_results(loss_values,val_loss_values, epochs, **plot_para) 
</span></span></code></pre></div><p>Following figure shows the accuracy improvemet.<figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/2.png></figure></p><p>Now let us investigate what will happen, if we increase the depth of the neural network. For example we will try in the layer-1 400 neurons, in the layer-2 200 neurons and finally the layer-3 is softmax.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>model = models.Sequential()
</span></span><span style=display:flex><span>model.add(layers.Dense(400, activation=<span style=font-style:italic>&#39;relu&#39;</span>, input_shape=(784,)))
</span></span><span style=display:flex><span>model.add(layers.Dense(200, activation=<span style=font-style:italic>&#39;relu&#39;</span>))
</span></span><span style=display:flex><span>model.add(layers.Dense(10, activation=<span style=font-style:italic>&#39;softmax&#39;</span>))
</span></span><span style=display:flex><span><span style=font-style:italic># sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)         </span>
</span></span><span style=display:flex><span>model.compile(optimizer=<span style=font-style:italic>&#39;adam&#39;</span>, loss=<span style=font-style:italic>&#39;sparse_categorical_crossentropy&#39;</span>, metrics=[<span style=font-style:italic>&#39;accuracy&#39;</span>])
</span></span><span style=display:flex><span><span style=font-style:italic>#model.compile(loss=&#39;mean_squared_error&#39;, optimizer=sgd)</span>
</span></span><span style=display:flex><span>start_time = time.time()
</span></span><span style=display:flex><span>history = model.fit(x_train, y_train, epochs=20, batch_size = 256, validation_split=0.1)
</span></span><span style=display:flex><span>duration = time.time() - start_time
</span></span><span style=display:flex><span>print(<span style=font-style:italic>&#34;Total time is: &#34;</span>, round(duration, 2), <span style=font-style:italic>&#34;seconds&#34;</span>)
</span></span></code></pre></div><p>We can observe a slight over-fitting in the case.<figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/3.png></figure></p><p><strong>Overfitting</strong></p><p>In ML/DL, the overfitting means where a model has been trained too well on the training data, so that it even captures the noises or random fluctuations in the data, instead of the underlying pattern. Because of this poor generalization and the trained model may not perform well on unseen data. Overfitting can occur when a model has too many parameters relative to the number of training examples, or when a model is too complex for the given problem. There are few ways we can handle this situation; such as early stopping, regularization, dropout or cross-validation. Following table shows how to spot over and underfitting during the training process.</p><figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/fit.png></figure><p>Let us increase the model complexity even further. May be 4 layer networks configuration this time. In the lasyer-1 600 neurons; layer-2 400 neurons, layer-3 200 neurons and the layer-4 is softmax.
Layer 1- 600 Neurons, Layer 2- 400 Neurons, Layer 3- 200 Neurons , Layer 4 – Softmax.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>model = models.Sequential()
</span></span><span style=display:flex><span>model.add(layers.Dense(600, activation=<span style=font-style:italic>&#39;relu&#39;</span>, input_shape=(784,)))
</span></span><span style=display:flex><span>model.add(layers.Dense(400, activation=<span style=font-style:italic>&#39;relu&#39;</span>))
</span></span><span style=display:flex><span>model.add(layers.Dense(200, activation=<span style=font-style:italic>&#39;relu&#39;</span>))
</span></span><span style=display:flex><span>model.add(layers.Dense(10, activation=<span style=font-style:italic>&#39;softmax&#39;</span>))
</span></span><span style=display:flex><span><span style=font-style:italic># sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)         </span>
</span></span><span style=display:flex><span>model.compile(optimizer=<span style=font-style:italic>&#39;adam&#39;</span>, loss=<span style=font-style:italic>&#39;sparse_categorical_crossentropy&#39;</span>, metrics=[<span style=font-style:italic>&#39;accuracy&#39;</span>])
</span></span><span style=display:flex><span><span style=font-style:italic>#model.compile(loss=&#39;mean_squared_error&#39;, optimizer=sgd)</span>
</span></span><span style=display:flex><span>start_time = time.time()
</span></span><span style=display:flex><span>history = model.fit(x_train, y_train, epochs=20, batch_size = 256, validation_split=0.1)
</span></span><span style=display:flex><span>duration = time.time() - start_time
</span></span><span style=display:flex><span>print(<span style=font-style:italic>&#34;Total time is: &#34;</span>, round(duration, 2), <span style=font-style:italic>&#34;seconds&#34;</span>)
</span></span></code></pre></div><p>We can clearly see now, the overfitting increases drastically with the number of layer and neurons. This is an important hyperparameter to workaround.</p><figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/overfit.png></figure><p>If we test different regularization techniques, such as L1, L2 and dropout, it is clear that the L1 regularization perfomance is quite bad and L2 performence is very well.<figure class=center><img src=https://AIThoughtLab.github.io/ThinkingAI/images/regularization.png></figure></p><p>Dropout of <strong>0.2</strong> is performing well with accuracy of 93% on the test data compared to the <strong>0.5</strong> dropout.</p></div><footer></footer></article></section></div><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></main><script src=https://AIThoughtLab.github.io/ThinkingAI/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>